{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Epic: Migrate AI integration to Gemini 2.5 Pro",
        "description": "Replace OpenAI with @google/genai, use model `gemini-2.5-pro`, add streaming + JSON mode, secure key handling in Electron main via IPC, update tests/docs.",
        "details": "Scope: main-process client, streaming to renderer, JSON mode for summaries, settings UI for key, repo-wide replacement, security posture per Electron docs.",
        "testStrategy": "Unit tests for client wrapper, IPC handlers, and JSON parsing; integration smoke for streaming UI.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Spike: Set up @google/genai client and streaming in Electron main",
        "description": "Install @google/genai, initialize GoogleGenAI with GEMINI_API_KEY, implement generateContentStream wrapper and log chunks.",
        "details": "Create a main-process module `src/main/ai/gemini.ts` exporting `stream(prompt)` and `json(prompt)` using model `gemini-2.5-pro`. Wire minimal error handling.",
        "testStrategy": "Unit: mock SDK; assert streaming iteration and error handling.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "IPC bridge: expose Gemini generate + streaming to renderer",
        "description": "Add ipcMain handler and event channel; preload contextBridge exposes invoke and listener; validate sender; no key leakage.",
        "details": "Add channels: `gemini:generate` (invoke) and `gemini:stream` (event). Update preload to expose minimal API. Update window creation to set security flags.",
        "testStrategy": "Unit: mock ipcMain handlers; integration: preload exposure shape.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Renderer: integrate streaming UI and buffering",
        "description": "Adapt chat/editor to consume stream chunks, buffer safely, finalize completion, and handle errors/cancellations.",
        "details": "Add a React hook/useGeminiStream for subscription; ensure UI state transitions match existing behavior; debounce renders.",
        "testStrategy": "Integration: mount component and simulate chunked updates; ensure final text matches.",
        "status": "done",
        "dependencies": [
          "3"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "JSON mode: structured summaries and parsing",
        "description": "Add generationConfig.responseMimeType='application/json' for summary endpoints; robust JSON parse with fallback and user feedback.",
        "details": "Create helper `generateJson(prompt)` with schema-in-prompt; surface parse errors with toasts; add typing to parsed objects.",
        "testStrategy": "Unit: valid/invalid JSON responses; UI: shows error toast when invalid.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Settings: Gemini API key input + validation",
        "description": "Add UI to store/validate GEMINI_API_KEY (main process storage) and block AI actions when missing/invalid.",
        "details": "Reword settings from OpenAI to Gemini; add key test ping; never expose key to renderer logs; update README copy.",
        "testStrategy": "Unit: validation logic; UI: disables actions without key; e2e: happy path store + use.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Repo-wide replacement: remove OpenAI, wire Gemini services",
        "description": "Remove `openai` package and usages; replace with Gemini service calls in contexts and handlers; update typings.",
        "details": "Search and replace: chat/completions â†’ generateContent/stream; update prompt builders; revisit rate-limit/backoff handlers.",
        "testStrategy": "Run full test suite; smoke run on all AI features; ensure no OpenAI imports remain.",
        "status": "done",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Security & CSP: enforce Electron best practices",
        "description": "Verify contextIsolation, nodeIntegration, preload exposure, and add/adjust CSP. Validate IPC sender origins; sanitize inputs.",
        "details": "Audit BrowserWindow creation, preload API surface, and enable strict CSP for renderer assets.",
        "testStrategy": "Manual audit checklist + unit tests for preload API shape; verify no node APIs in renderer.",
        "status": "done",
        "dependencies": [
          "2",
          "3"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Tests and CI adjustments",
        "description": "Update unit/integration tests for Gemini workflows; ensure CI scripts keep lint, type-check, packaging, and tests green.",
        "details": "Add mocks for @google/genai; update jest setup if needed; ensure streaming tests are deterministic.",
        "testStrategy": "Jest unit tests for wrappers; integration tests for streaming buffer and JSON parsing.",
        "status": "done",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Docs & release notes",
        "description": "Update README, AGENTS.md, and in-app copy to reference Gemini; add migration notes and release highlights.",
        "details": "Document settings changes, model defaults, and limitations; include references to Google docs.",
        "testStrategy": "Docs review checklist; ensure commands and paths resolve.",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "9"
        ],
        "priority": "low",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-26T16:01:27.799Z",
      "description": "Default tasks context",
      "updated": "2025-08-27T16:31:50.326Z"
    }
  },
  "feature-gemini-embeddings": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Embedding Provider Interface and Implement Gemini Provider (Initial)",
        "description": "Define a common interface for embedding providers and create the initial implementation for Google Gemini's `text-embedding-004` model. This task focuses on the core logic for generating embeddings.",
        "details": "1. Define `IEmbeddingProvider` interface in the main process with `embed(text: string): Promise<number[]>` and `embedBatch(texts: string[]): Promise<number[][]>` signatures.\n2. Implement `GeminiEmbeddingProvider` class in the main process, utilizing the `@google/genai` library or direct REST calls for `text-embedding-004`.\n3. Ensure all network calls are confined to the main Electron process.\n4. Handle basic API key usage for Gemini (validation will be in a later task).",
        "testStrategy": "Unit tests for `GeminiEmbeddingProvider` to verify `embed` and `embedBatch` functionality, mocking network requests to the Gemini API. Test with single and multiple text inputs.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define `IEmbeddingProvider` Interface",
            "description": "Create the `IEmbeddingProvider` interface in the main Electron process, specifying the `embed` and `embedBatch` methods.",
            "dependencies": [],
            "details": "Define `IEmbeddingProvider` in the main process with `embed(text: string): Promise<number[]>` and `embedBatch(texts: string[]): Promise<number[][]>` signatures.",
            "status": "pending",
            "testStrategy": "N/A (interface definition doesn't have direct unit tests; its correctness will be verified by implementations)."
          },
          {
            "id": 2,
            "title": "Set up Gemini API Client and Basic Configuration",
            "description": "Integrate the `@google/genai` library or prepare for direct REST calls, and establish a mechanism for basic API key usage within the main process.",
            "dependencies": [],
            "details": "Install `@google/genai` library. Create a utility or module in the main process to initialize the Gemini client with an API key. Ensure API key handling is basic, as validation is for a later task.",
            "status": "pending",
            "testStrategy": "Unit tests for the API client initialization, ensuring it can be instantiated with a provided API key."
          },
          {
            "id": 3,
            "title": "Implement `GeminiEmbeddingProvider.embed` Method",
            "description": "Implement the `embed` method within the `GeminiEmbeddingProvider` class to generate embeddings for a single text string using the `text-embedding-004` model.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Create the `GeminiEmbeddingProvider` class implementing `IEmbeddingProvider`. Implement the `embed(text: string): Promise<number[]>` method using the configured Gemini client to call the `text-embedding-004` model. Ensure network calls are confined to the main process.",
            "status": "pending",
            "testStrategy": "Unit tests for `GeminiEmbeddingProvider.embed`, mocking network requests to the Gemini API to verify correct input/output and error handling for single text inputs."
          },
          {
            "id": 4,
            "title": "Implement `GeminiEmbeddingProvider.embedBatch` Method",
            "description": "Implement the `embedBatch` method within the `GeminiEmbeddingProvider` class to efficiently generate embeddings for an array of text strings using the `text-embedding-004` model.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3"
            ],
            "details": "Implement the `embedBatch(texts: string[]): Promise<number[][]>` method in `GeminiEmbeddingProvider`, leveraging the Gemini API's batch capabilities for `text-embedding-004`. Ensure network calls are confined to the main process.",
            "status": "pending",
            "testStrategy": "Unit tests for `GeminiEmbeddingProvider.embedBatch`, mocking network requests to the Gemini API to verify correct input/output and error handling for multiple text inputs."
          },
          {
            "id": 5,
            "title": "Integrate and Verify Gemini Provider in Main Process",
            "description": "Ensure the `GeminiEmbeddingProvider` is correctly instantiated and accessible within the main Electron process, and confirm that all network calls are strictly confined to this process.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3",
              "1.4"
            ],
            "details": "Instantiate `GeminiEmbeddingProvider` in a suitable location within the main process. Add basic logging or a simple test call to confirm it can be used. Explicitly verify that no network calls are initiated from renderer processes related to this provider.",
            "status": "pending",
            "testStrategy": "Integration test to instantiate `GeminiEmbeddingProvider` in the main process and make a live (or mocked) call to `embed` and `embedBatch` to confirm functionality and process confinement."
          }
        ]
      },
      {
        "id": 2,
        "title": "Adapt Ollama to New Provider Interface",
        "description": "Refactor the existing Ollama embedding generation logic to conform to the newly defined `IEmbeddingProvider` interface, ensuring it can be swapped interchangeably with other providers.",
        "details": "1. Create an `OllamaEmbeddingProvider` class in the main process.\n2. Migrate the existing Ollama embedding generation logic (likely using `mxbai-embed-large`) into this new class.\n3. Ensure `OllamaEmbeddingProvider` implements the `IEmbeddingProvider` interface, providing `embed` and `embedBatch` methods.",
        "testStrategy": "Unit tests for `OllamaEmbeddingProvider` to ensure it correctly generates embeddings and conforms to the `IEmbeddingProvider` interface, mocking any external Ollama service calls if necessary.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize OllamaEmbeddingProvider Class Structure",
            "description": "Create the `OllamaEmbeddingProvider` class file in the main process, defining its basic structure and declaring its intent to implement `IEmbeddingProvider`. Include stub implementations for `embed` and `embedBatch` methods.",
            "dependencies": [],
            "details": "Create `ollamaEmbeddingProvider.ts` (or similar) in the main process. Define `class OllamaEmbeddingProvider implements IEmbeddingProvider { ... }`. Add empty or placeholder `embed` and `embedBatch` methods to satisfy the interface initially.",
            "status": "pending",
            "testStrategy": "N/A (structural setup)"
          },
          {
            "id": 2,
            "title": "Implement Single Embedding (`embed`) for Ollama",
            "description": "Migrate the existing Ollama single embedding generation logic (likely using `mxbai-embed-large`) into the `embed` method of the `OllamaEmbeddingProvider` class.",
            "dependencies": [
              "2.1"
            ],
            "details": "Identify the current Ollama embedding call for a single text input. Adapt this logic to fit within the `embed(text: string): Promise<number[]>` signature, making necessary calls to the Ollama service and handling its response.",
            "status": "pending",
            "testStrategy": "Unit tests for `embed` method, mocking Ollama service calls to verify correct embedding generation for single inputs."
          },
          {
            "id": 3,
            "title": "Implement Batch Embeddings (`embedBatch`) for Ollama",
            "description": "Develop the `embedBatch` method within `OllamaEmbeddingProvider` to handle multiple text inputs, leveraging Ollama's capabilities for batch processing if available, or adapting the single embedding logic for efficient batching.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Implement `embedBatch(texts: string[]): Promise<number[][]>`. This might involve iterating and calling the single `embed` logic, or using a specific Ollama API for batch requests if it exists and is more efficient. Ensure error handling for batch operations.",
            "status": "pending",
            "testStrategy": "Unit tests for `embedBatch` method, mocking Ollama service calls to verify correct embedding generation for multiple inputs and proper handling of batch sizes."
          },
          {
            "id": 4,
            "title": "Ensure `IEmbeddingProvider` Interface Conformance",
            "description": "Review and ensure that the `OllamaEmbeddingProvider` class fully and correctly implements the `IEmbeddingProvider` interface, including method signatures, return types, and overall contract.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3"
            ],
            "details": "Perform a code review to confirm `OllamaEmbeddingProvider` explicitly implements `IEmbeddingProvider` and that `embed(text: string): Promise<number[]>` and `embedBatch(texts: string[]): Promise<number[][]>` methods match the interface definition precisely. Address any type mismatches or missing implementations.",
            "status": "pending",
            "testStrategy": "Type checking during compilation and basic structural tests to confirm interface implementation."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for OllamaEmbeddingProvider",
            "description": "Write comprehensive unit tests for the `OllamaEmbeddingProvider` class to verify its `embed` and `embedBatch` methods correctly generate embeddings and adhere to the `IEmbeddingProvider` interface. Mock external Ollama service calls.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4"
            ],
            "details": "Create a test file (e.g., `ollamaEmbeddingProvider.test.ts`). Write tests for `embed` with various inputs. Write tests for `embedBatch` with single and multiple inputs. Use a mocking library (e.g., Jest mocks) to simulate Ollama API responses, ensuring the tests are isolated and fast.",
            "status": "pending",
            "testStrategy": "Unit tests for `OllamaEmbeddingProvider` to ensure it correctly generates embeddings and conforms to the `IEmbeddingProvider` interface, mocking any external Ollama service calls if necessary."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Provider Factory and Integrate with Main Process Settings",
        "description": "Create a factory to dynamically select the appropriate embedding provider based on application settings and integrate this selection mechanism into the main process.",
        "details": "1. Develop an `EmbeddingProviderFactory` in the main process that takes a `pileAIProvider` string ('gemini' | 'ollama') and returns an instance of the corresponding `IEmbeddingProvider`.\n2. Integrate this factory into the main process's core logic where embedding providers are instantiated (e.g., within the `pileEmbeddings` module or a similar service).\n3. Ensure the main process can read the `pileAIProvider` setting to determine which provider to use.",
        "testStrategy": "Unit tests for `EmbeddingProviderFactory` to verify it returns the correct provider instance based on the input string. Test with both 'gemini' and 'ollama' inputs.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define IEmbeddingProvider Interface",
            "description": "Create the `IEmbeddingProvider` interface in the main process, specifying methods like `embed` and `embedBatch` that all embedding providers must implement.",
            "dependencies": [],
            "details": "Define the `IEmbeddingProvider` interface with methods such as `embed(text: string): Promise<number[]>` and `embedBatch(texts: string[]): Promise<number[][]>`.",
            "status": "pending",
            "testStrategy": "N/A (Interfaces are not directly tested, but their implementation is)."
          },
          {
            "id": 2,
            "title": "Implement GeminiEmbeddingProvider",
            "description": "Create the `GeminiEmbeddingProvider` class, implementing the `IEmbeddingProvider` interface, to interact with the Gemini API for embedding generation.",
            "dependencies": [
              "3.1"
            ],
            "details": "Create a `GeminiEmbeddingProvider` class in the main process. Implement the `embed` and `embedBatch` methods using the Gemini API.",
            "status": "pending",
            "testStrategy": "Unit tests for `GeminiEmbeddingProvider` to ensure it correctly generates embeddings and conforms to the `IEmbeddingProvider` interface, mocking Gemini API calls."
          },
          {
            "id": 3,
            "title": "Develop EmbeddingProviderFactory Structure",
            "description": "Create the `EmbeddingProviderFactory` class in the main process, including a method to dynamically select and instantiate the correct `IEmbeddingProvider` based on a given string identifier ('gemini' or 'ollama').",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Create an `EmbeddingProviderFactory` class. Implement a method (e.g., `getProvider(providerName: 'gemini' | 'ollama'): IEmbeddingProvider`) that uses a switch or map to return an instance of `GeminiEmbeddingProvider` or `OllamaEmbeddingProvider`.",
            "status": "pending",
            "testStrategy": "Unit tests for `EmbeddingProviderFactory` to verify it returns the correct provider instance (e.g., `OllamaEmbeddingProvider` or `GeminiEmbeddingProvider`) based on the input string, ensuring it throws an error for unknown providers."
          },
          {
            "id": 4,
            "title": "Implement Settings Reading for pileAIProvider",
            "description": "Implement the logic in the main process to read the `pileAIProvider` setting from the application's configuration to determine which embedding provider to use.",
            "dependencies": [],
            "details": "Identify or create a mechanism within the main process to access application settings. Implement a function or module to retrieve the value of `pileAIProvider` (e.g., from a config file, environment variable, or secure storage).",
            "status": "pending",
            "testStrategy": "Unit tests for the settings reading mechanism to ensure it correctly retrieves the `pileAIProvider` value, including default values or error handling for missing settings."
          },
          {
            "id": 5,
            "title": "Integrate EmbeddingProviderFactory into Main Process",
            "description": "Integrate the `EmbeddingProviderFactory` into the main process's core logic, specifically within modules like `pileEmbeddings` or a similar service, to dynamically instantiate the selected embedding provider.",
            "dependencies": [
              "3.3",
              "3.4"
            ],
            "details": "Modify the `pileEmbeddings` module or relevant service to use the `EmbeddingProviderFactory`. Instead of directly instantiating a provider, call `factory.getProvider(pileAIProviderSetting)` to obtain the active `IEmbeddingProvider` instance.",
            "status": "pending",
            "testStrategy": "Integration tests to verify that the main process correctly uses the `EmbeddingProviderFactory` to instantiate the provider specified by `pileAIProvider` setting, and that subsequent embedding operations use this dynamically selected provider."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop Settings UI for Provider Selection and Gemini API Key Management",
        "description": "Implement the user interface elements for selecting the embedding provider and securely managing the Gemini API key, ensuring secure storage and validation.",
        "details": "1. Add UI components in the renderer process (e.g., in a settings panel) to allow users to choose between 'Gemini' and 'Ollama' as the embedding provider.\n2. Add an input field for the `GEMINI_API_KEY` in the settings UI.\n3. Implement IPC communication between the renderer and main processes for saving and retrieving the `GEMINI_API_KEY` securely.\n4. In the main process, implement logic to store the `GEMINI_API_KEY` using existing secure storage mechanisms.\n5. Implement a lightweight validation call to the Gemini API in the main process when the key is saved/updated, and surface any errors back to the renderer UI.",
        "testStrategy": "Manual testing of the UI for provider selection and key input. Unit tests for IPC handlers in the main process for key storage and validation. Integration tests for key validation with mocked Gemini API responses.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Provider Selection UI",
            "description": "Add UI components in the renderer process (e.g., in a settings panel) to allow users to choose between 'Gemini' and 'Ollama' as the embedding provider.",
            "dependencies": [],
            "details": "Add radio buttons or a dropdown in the settings panel for 'Gemini' and 'Ollama' provider selection. Ensure the selected provider state can be managed locally.",
            "status": "pending",
            "testStrategy": "Manually verify the presence and functionality of the provider selection UI elements. Ensure selection changes are reflected."
          },
          {
            "id": 2,
            "title": "Implement Gemini API Key Input UI",
            "description": "Add an input field for the `GEMINI_API_KEY` in the settings UI, ensuring it's visually distinct and potentially masked for security.",
            "dependencies": [],
            "details": "Create a text input field in the settings panel for the `GEMINI_API_KEY`. Consider using a password type input for masking. Include a save button for the key.",
            "status": "pending",
            "testStrategy": "Manually verify the presence and functionality of the API key input field. Test input masking if implemented."
          },
          {
            "id": 3,
            "title": "Implement Secure API Key Storage in Main Process",
            "description": "In the main process, implement logic to securely store and retrieve the `GEMINI_API_KEY` using existing secure storage mechanisms (e.g., Electron's `keytar` or similar).",
            "dependencies": [],
            "details": "Implement functions in the main process (e.g., `saveGeminiApiKey`, `getGeminiApiKey`) that interact with the application's secure storage mechanism to persist and retrieve the `GEMINI_API_KEY`.",
            "status": "pending",
            "testStrategy": "Unit tests for the key storage and retrieval logic in the main process, ensuring keys are stored and retrieved securely and correctly."
          },
          {
            "id": 4,
            "title": "Implement Gemini API Key Validation Logic",
            "description": "Implement a lightweight validation call to the Gemini API in the main process. This logic should take an API key, make a test call (e.g., list models), and return a boolean indicating validity or an error message.",
            "dependencies": [],
            "details": "Create a function `validateGeminiApiKey(key: string): Promise<boolean | string>` in the main process that makes a simple, low-cost API call to Gemini (e.g., `listModels`) and handles success/failure responses.",
            "status": "pending",
            "testStrategy": "Unit tests for the key validation logic, mocking Gemini API responses for success, invalid key errors, and other network-related failures."
          },
          {
            "id": 5,
            "title": "Implement IPC and Integrate UI with Main Process Logic",
            "description": "Implement IPC communication between the renderer and main processes. Connect the UI elements (provider selection, API key input) to the main process logic for saving/retrieving the `GEMINI_API_KEY`, triggering validation, and surfacing any errors back to the renderer UI.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4"
            ],
            "details": "Define IPC channels for saving/retrieving provider selection and `GEMINI_API_KEY`. Implement `ipcRenderer.invoke` in the renderer and `ipcMain.handle` in the main process. In the main process IPC handler for saving the key, call secure storage (4.3) and then validation logic (4.4), surfacing results/errors to the renderer. Wire up UI elements (4.1, 4.2) to use these IPC calls.",
            "status": "pending",
            "testStrategy": "Manual testing of the UI for provider selection and key input, ensuring saving, retrieval, and validation work correctly. Unit tests for IPC handlers in the main process. Integration tests for key validation with mocked Gemini API responses, verifying error surfacing."
          }
        ]
      },
      {
        "id": 5,
        "title": "Refactor Pile Embeddings Module to Use Provider Abstraction and Batching",
        "description": "Update the core `pileEmbeddings` module to leverage the new provider abstraction and introduce batch processing for improved performance during embedding generation.",
        "details": "1. Modify `pileEmbeddings.generateEmbedding` to use the `embed` method of the currently selected `IEmbeddingProvider` instance obtained from the factory.\n2. Update `walkAndGenerateEmbeddings` to utilize the `embedBatch` method of the selected provider, processing multiple texts concurrently.\n3. Introduce a configurable batch size for `walkAndGenerateEmbeddings` to optimize performance and respect API limits.",
        "testStrategy": "Unit tests for `pileEmbeddings` module, ensuring it correctly delegates to the selected provider. Integration tests for `walkAndGenerateEmbeddings` to verify batch processing and performance gains, using mocked provider responses.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize `pileEmbeddings` Module with Provider Factory",
            "description": "Modify the `pileEmbeddings` module's initialization or constructor to obtain an instance of `IEmbeddingProvider` from the `EmbeddingProviderFactory` based on application settings. This instance will be made available for subsequent embedding generation methods.",
            "dependencies": [],
            "details": "Implement logic within the `pileEmbeddings` module (e.g., in its constructor or an initialization method) to call `EmbeddingProviderFactory.getProvider(settings.pileAIProvider)` and store the returned `IEmbeddingProvider` instance as a member variable.",
            "status": "pending",
            "testStrategy": "Unit tests for the `pileEmbeddings` module's initialization logic, ensuring it correctly obtains and stores an `IEmbeddingProvider` instance from a mocked `EmbeddingProviderFactory`."
          },
          {
            "id": 2,
            "title": "Refactor `pileEmbeddings.generateEmbedding` for Single Text",
            "description": "Update the `pileEmbeddings.generateEmbedding` method to delegate its call to the `embed` method of the `IEmbeddingProvider` instance obtained in Subtask 5.1.",
            "dependencies": [
              "5.1"
            ],
            "details": "Change the implementation of `pileEmbeddings.generateEmbedding(text: string)` to call `this.provider.embed(text)` where `this.provider` is the `IEmbeddingProvider` instance made available by Subtask 5.1.",
            "status": "pending",
            "testStrategy": "Unit tests for `pileEmbeddings.generateEmbedding`, verifying it correctly delegates to the `embed` method of the internal `IEmbeddingProvider` instance, using a mocked provider."
          },
          {
            "id": 3,
            "title": "Implement Core Batching Logic in `walkAndGenerateEmbeddings`",
            "description": "Modify `walkAndGenerateEmbeddings` to collect multiple text inputs into batches and then call the `embedBatch` method of the `IEmbeddingProvider` instance for each batch. Initially, this can use a hardcoded or default batch size.",
            "dependencies": [
              "5.1"
            ],
            "details": "Introduce a loop or mechanism within `walkAndGenerateEmbeddings` to accumulate texts into an array. Once the array reaches a certain size (e.g., a hardcoded default), call `this.provider.embedBatch(batchArray)` and process the results, handling asynchronous batch processing.",
            "status": "pending",
            "testStrategy": "Unit tests for `walkAndGenerateEmbeddings` focusing on the batching mechanism, ensuring texts are correctly grouped and `embedBatch` is called with the expected batches, using a mocked provider and a default batch size."
          },
          {
            "id": 4,
            "title": "Integrate Configurable Batch Size into `walkAndGenerateEmbeddings`",
            "description": "Introduce a configurable batch size setting for `walkAndGenerateEmbeddings` and ensure the batching mechanism implemented in Subtask 5.3 utilizes this setting to optimize performance and respect API limits.",
            "dependencies": [
              "5.3"
            ],
            "details": "Read the `batchSize` from application settings or a module-specific configuration. Update the batching loop in `walkAndGenerateEmbeddings` to use this configurable value when determining when to call `embedBatch`.",
            "status": "pending",
            "testStrategy": "Unit tests for `walkAndGenerateEmbeddings` to verify that the configurable batch size is correctly read and applied to the batching logic, using a mocked provider and different batch size configurations."
          },
          {
            "id": 5,
            "title": "Comprehensive Testing and Performance Verification",
            "description": "Develop and execute unit tests for the `pileEmbeddings` module to ensure correct delegation to the selected provider. Implement integration tests for `walkAndGenerateEmbeddings` to verify the batch processing logic, configurable batch size, and measure performance gains using mocked provider responses.",
            "dependencies": [
              "5.2",
              "5.4"
            ],
            "details": "Create unit tests for `pileEmbeddings.generateEmbedding` and `walkAndGenerateEmbeddings` to confirm they correctly interact with the `IEmbeddingProvider`. Write integration tests for `walkAndGenerateEmbeddings` that mock `embedBatch` calls to simulate different batch sizes and measure the overall execution time and correctness of embedding generation, ensuring performance improvements.",
            "status": "pending",
            "testStrategy": "Integration tests for `walkAndGenerateEmbeddings` to verify end-to-end batch processing, including correct handling of various batch sizes and performance measurement, using mocked provider responses. Unit tests for `pileEmbeddings` module to ensure overall correct delegation."
          }
        ]
      },
      {
        "id": 6,
        "title": "Update `embeddings.json` Schema and Implement Backward Compatibility",
        "description": "Modify the persistence layer for embeddings to support a new, more descriptive schema in `embeddings.json` while ensuring backward compatibility with existing legacy files.",
        "details": "1. Update the `embeddings.json` read/write logic to persist data in the new schema: `{ version, model, provider, dims, createdAt, items: [ [path, vector] ] }`.\n2. Implement logic to detect and read the legacy `[ [path, vector] ]` format.\n3. Ensure that when a legacy `embeddings.json` is read, it is automatically converted to the new schema upon the next save operation.",
        "testStrategy": "Unit tests for the `embeddings.json` parser/writer, covering both new schema serialization/deserialization and backward compatibility for reading legacy formats. Test automatic migration on save.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define New `embeddings.json` Schema Data Model",
            "description": "Create the data structures (e.g., TypeScript interfaces or classes) that represent the new `embeddings.json` schema, including fields like `version`, `model`, `provider`, `dims`, `createdAt`, and `items`.",
            "dependencies": [],
            "details": "Define the `EmbeddingsFile` interface/type: `{ version: string; model: string; provider: string; dims: number; createdAt: string; items: [string, number[]][]; }`. Also define the `LegacyEmbeddingsFile` type: `[string, number[]][]` for clear differentiation.",
            "status": "pending",
            "testStrategy": "N/A (This is a definition task; type checks in consuming code will implicitly validate it)."
          },
          {
            "id": 2,
            "title": "Implement New Schema Write Logic",
            "description": "Modify the `embeddings.json` persistence layer to write embedding data in the newly defined descriptive schema format.",
            "dependencies": [
              "6.1"
            ],
            "details": "Update the serialization logic to convert the in-memory embedding data into the new `{ version, model, provider, dims, createdAt, items: [ [path, vector] ] }` structure before writing to `embeddings.json`. Ensure `version` is set correctly (e.g., '1.0.0') and other metadata fields are populated.",
            "status": "pending",
            "testStrategy": "Unit tests to verify that an in-memory representation of embeddings is correctly serialized and written to a file in the new schema format, including all metadata fields."
          },
          {
            "id": 3,
            "title": "Implement New Schema Read Logic",
            "description": "Implement the deserialization logic to read and parse `embeddings.json` files that conform to the new, descriptive schema.",
            "dependencies": [
              "6.1"
            ],
            "details": "Update the deserialization logic to parse `embeddings.json` files that contain the new schema fields (`version`, `model`, `provider`, etc.) into the application's in-memory data structures, ensuring all metadata is correctly extracted.",
            "status": "pending",
            "testStrategy": "Unit tests to verify that a file written in the new schema format can be correctly read and deserialized into the application's in-memory data structures, preserving all data and metadata."
          },
          {
            "id": 4,
            "title": "Implement Legacy Schema Detection and Read Logic",
            "description": "Add logic to the `embeddings.json` persistence layer to detect and correctly read files that are in the old, legacy `[ [path, vector] ]` format.",
            "dependencies": [
              "6.1"
            ],
            "details": "Implement a mechanism (e.g., checking for the presence of the `version` field or type checking the root element) to determine if a loaded `embeddings.json` file is in the legacy `[ [path, vector] ]` format. If legacy, parse it into the application's in-memory data structure, potentially marking it as 'legacy' for later migration.",
            "status": "pending",
            "testStrategy": "Unit tests to verify that a file in the legacy `[ [path, vector] ]` format is correctly detected as legacy and successfully read into the application's in-memory data structures."
          },
          {
            "id": 5,
            "title": "Implement Automatic Legacy to New Schema Migration on Save",
            "description": "Ensure that any `embeddings.json` file initially read in the legacy format is automatically converted and saved in the new schema format upon the next save operation.",
            "dependencies": [
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "After a legacy `embeddings.json` file is read (as per 6.4), the in-memory data should be internally flagged as needing migration. When a save operation is triggered, if this flag is set, the data must be written using the new schema write logic (as per 6.2), effectively upgrading the file on disk.",
            "status": "pending",
            "testStrategy": "Integration tests: Read a legacy `embeddings.json` file, perform an operation that triggers a save (even if no data changed), then verify that the `embeddings.json` file on disk is now in the new schema format with all appropriate metadata populated."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement 'Regenerate Embeddings' Action and Progress Reporting",
        "description": "Add a user-initiated action to regenerate all embeddings for a pile using the currently selected provider, with visual feedback on the progress.",
        "details": "1. Implement an IPC handler in the main process for a 'Regenerate embeddings' command.\n2. This command should clear the in-memory embeddings map for the current pile.\n3. Trigger `walkAndGenerateEmbeddings` (from Task 5) to rebuild all embeddings using the currently selected provider.\n4. Ensure the newly generated embeddings are saved to `embeddings.json` with the updated schema (from Task 6).\n5. Implement progress reporting via IPC from the main process to the renderer, allowing for UI updates (e.g., a toast notification or log messages).",
        "testStrategy": "Integration test for the 'Regenerate embeddings' flow, verifying that embeddings are cleared, rebuilt with the correct provider, and saved. Test progress reporting via mocked IPC listeners.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Main Process IPC Handler and Clear Embeddings",
            "description": "Create an IPC handler in the main process to receive a 'regenerate-embeddings' command. Upon receiving the command, clear the in-memory embeddings map for the current pile to prepare for regeneration.",
            "dependencies": [],
            "details": "Implement `ipcMain.handle('regenerate-embeddings', ...)` in the main process. Within this handler, add logic to clear the data structure holding the current pile's embeddings in memory.",
            "status": "pending",
            "testStrategy": "Unit test the IPC handler to ensure it's registered and correctly clears the in-memory map when invoked. Mock the in-memory map for verification."
          },
          {
            "id": 2,
            "title": "Trigger `walkAndGenerateEmbeddings` for Regeneration",
            "description": "Within the 'regenerate-embeddings' IPC handler, after clearing the existing embeddings, trigger the `walkAndGenerateEmbeddings` function (from Task 5) to rebuild all embeddings for the current pile using the currently selected provider.",
            "dependencies": [
              "7.1"
            ],
            "details": "Call `walkAndGenerateEmbeddings` from the main process's IPC handler. Ensure it correctly utilizes the `IEmbeddingProvider` instance obtained via the provider factory (from Task 3) based on the settings (from Task 4).",
            "status": "pending",
            "testStrategy": "Integration test the IPC handler to verify that `walkAndGenerateEmbeddings` is called. Mock `walkAndGenerateEmbeddings` to ensure it's invoked with the correct context and parameters."
          },
          {
            "id": 3,
            "title": "Persist Regenerated Embeddings to Disk",
            "description": "After `walkAndGenerateEmbeddings` completes, ensure that the newly generated embeddings are saved to the `embeddings.json` file, adhering to the updated schema defined in Task 6.",
            "dependencies": [
              "7.2"
            ],
            "details": "Add logic in the main process, following the completion of `walkAndGenerateEmbeddings`, to serialize and write the complete set of regenerated embeddings to the `embeddings.json` file, respecting the schema from Task 6.",
            "status": "pending",
            "testStrategy": "Integration test the full regeneration flow, verifying that `embeddings.json` is updated with the new embeddings and the correct schema. This may involve reading and parsing the file after the operation."
          },
          {
            "id": 4,
            "title": "Implement Main Process Progress Reporting via IPC",
            "description": "Integrate progress tracking into the `walkAndGenerateEmbeddings` process within the main process. Send periodic progress updates (e.g., total items, items processed, percentage complete) via IPC to the renderer process.",
            "dependencies": [
              "7.2"
            ],
            "details": "Modify `walkAndGenerateEmbeddings` or its surrounding logic to emit progress events. Implement `ipcMain.send` or a similar mechanism to push these updates to the renderer process, including relevant progress data.",
            "status": "pending",
            "testStrategy": "Unit test the progress reporting logic in the main process, mocking `walkAndGenerateEmbeddings` to emit progress events and verifying that IPC messages are sent correctly with expected data."
          },
          {
            "id": 5,
            "title": "Develop Renderer UI for Progress Feedback",
            "description": "Implement UI elements in the renderer process (e.g., a toast notification, a progress bar, or log messages) to visually display the progress updates received from the main process during the embedding regeneration.",
            "dependencies": [
              "7.4"
            ],
            "details": "Create a new UI component or modify an existing one in the renderer process to listen for IPC messages related to embedding regeneration progress. Update the UI dynamically based on the received progress data.",
            "status": "pending",
            "testStrategy": "Manual testing of the UI. Integration test the renderer component by mocking IPC messages from the main process and asserting that the UI updates correctly (e.g., toast appears, progress bar fills)."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Robust Error Handling and Rate Limiting for Embedding Providers",
        "description": "Enhance the embedding providers with robust error handling, including exponential backoff for transient network issues and graceful handling of partial failures during batch processing.",
        "details": "1. Implement exponential backoff and retry logic within `GeminiEmbeddingProvider` (and potentially `OllamaEmbeddingProvider`) for transient errors (e.g., HTTP 429 Too Many Requests, 5xx server errors).\n2. Ensure `walkAndGenerateEmbeddings` can handle partial failures within a batch (e.g., some texts fail to embed) and continue processing the remaining items.\n3. Collect and surface a summary of any failures (e.g., count of failed items, error messages) at the end of a full regeneration process.",
        "testStrategy": "Unit tests for provider error handling, mocking API responses for 429/5xx errors and verifying retry logic. Integration tests for `walkAndGenerateEmbeddings` with mocked partial failures to ensure graceful continuation and error summary reporting.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Generic Exponential Backoff and Retry Mechanism",
            "description": "Develop a reusable utility or decorator for exponential backoff and retry logic that can be applied to asynchronous operations, specifically targeting transient network errors (e.g., HTTP 429, 5xx).",
            "dependencies": [],
            "details": "Create a generic function or class that wraps an async function, retrying it with increasing delays (exponential backoff) on specified transient error types (e.g., network errors, HTTP 429 Too Many Requests, 5xx server errors). Configure parameters such as maximum retry attempts, initial delay, and backoff factor.",
            "status": "pending",
            "testStrategy": "Unit tests for the retry utility, mocking successful and failing async calls to verify retry count, delay progression, and eventual success or failure after max retries. Test with different error types to ensure correct retry triggering."
          },
          {
            "id": 2,
            "title": "Integrate Retry Logic into GeminiEmbeddingProvider",
            "description": "Apply the newly implemented exponential backoff and retry mechanism to the network calls made by `GeminiEmbeddingProvider` to handle transient API errors.",
            "dependencies": [
              "8.1"
            ],
            "details": "Modify the `GeminiEmbeddingProvider`'s `embed` and `embedBatch` methods to utilize the generic exponential backoff and retry utility (from 8.1) for their underlying API calls to the Gemini service. Configure the retry logic to specifically target HTTP 429 (Too Many Requests) and 5xx server errors, as per parent task details.",
            "status": "pending",
            "testStrategy": "Unit tests for `GeminiEmbeddingProvider`, mocking Gemini API responses for 429/5xx errors and verifying that the retry logic is engaged, delays are applied, and the operation eventually succeeds or fails gracefully after maximum retries. Ensure no retries for non-transient errors."
          },
          {
            "id": 3,
            "title": "Enhance walkAndGenerateEmbeddings for Partial Batch Failure Handling",
            "description": "Modify the `walkAndGenerateEmbeddings` function to gracefully handle scenarios where `embedBatch` returns partial failures (some texts fail to embed) without halting the entire process, ensuring remaining items are processed.",
            "dependencies": [],
            "details": "Update the logic within `walkAndGenerateEmbeddings` (from Task 5) that processes results from `embedBatch`. Instead of failing the entire batch or process on a single item's failure, identify which specific texts within a batch failed to embed and continue processing the successful ones. This involves adapting to the `embedBatch` return type to differentiate successful and failed embeddings.",
            "status": "pending",
            "testStrategy": "Integration tests for `walkAndGenerateEmbeddings` with mocked `embedBatch` responses that simulate partial failures (e.g., some embeddings are null or error objects). Verify that the process continues for successful items in the batch and subsequent batches, and does not crash."
          },
          {
            "id": 4,
            "title": "Implement Failure Collection and Aggregation",
            "description": "Develop a mechanism within the embedding generation process (specifically `walkAndGenerateEmbeddings`) to collect details of all failed embedding attempts, including the original text, error message, and potentially the provider used.",
            "dependencies": [
              "8.3"
            ],
            "details": "Introduce a data structure (e.g., an array of objects) to accumulate comprehensive failure information during the execution of `walkAndGenerateEmbeddings`. Each entry should capture the original text that failed, the specific error message, and potentially the index or identifier of the failed item. This collection should be updated as partial failures are handled (from 8.3).",
            "status": "pending",
            "testStrategy": "Unit/Integration tests for `walkAndGenerateEmbeddings` (or a dedicated error collection module), mocking various failure scenarios (e.g., single item failure, multiple item failures across batches) and verifying that the correct and complete failure details are collected and aggregated."
          },
          {
            "id": 5,
            "title": "Surface Failure Summary to User Interface",
            "description": "Extend the 'Regenerate Embeddings' action (from Task 7) to surface the collected summary of embedding failures to the renderer process, allowing the UI to display this information to the user.",
            "dependencies": [
              "8.4"
            ],
            "details": "Modify the IPC communication for the 'Regenerate embeddings' command (from Task 7) to include the aggregated failure summary (from 8.4) as part of its completion message or a dedicated error reporting channel. Ensure the main process formats this summary appropriately for consumption by the renderer, which will then be responsible for displaying it.",
            "status": "pending",
            "testStrategy": "Integration tests for the 'Regenerate embeddings' flow, mocking embedding failures and verifying that the failure summary is correctly transmitted via IPC from the main process to a mocked renderer listener. Test that the summary contains expected details like count of failures and example error messages."
          }
        ]
      },
      {
        "id": 9,
        "title": "Develop Unit and Integration Tests",
        "description": "Create comprehensive unit and integration tests to ensure the correctness, reliability, and performance of the new embedding provider system and its integration.",
        "details": "1. Unit tests for `IEmbeddingProvider` implementations (Gemini, Ollama), `EmbeddingProviderFactory`, batching logic, and `embeddings.json` schema migration.\n2. Mock network calls for external APIs in unit tests.\n3. Integration test for the full `vectorSearch` path, ensuring it correctly uses the selected provider's embeddings for queries and returns relevant ranked results.\n4. Ensure existing tests are updated or adapted as necessary.",
        "testStrategy": "Execute all newly created unit and integration tests. Verify test coverage for critical paths. Ensure all acceptance criteria related to testing are met.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Unit Tests for IEmbeddingProvider Implementations",
            "description": "Create comprehensive unit tests for the concrete implementations of the `IEmbeddingProvider` interface, specifically for `GeminiEmbeddingProvider` and the assumed `OllamaEmbeddingProvider`, to verify their `embed` and `embedBatch` methods.",
            "dependencies": [],
            "details": "Implement tests to cover single text embedding and batch embedding functionality for both Gemini and Ollama providers. Ensure all external network calls to their respective APIs are mocked to isolate the unit under test. Verify correct input/output handling, error conditions, and adherence to the `IEmbeddingProvider` contract.",
            "status": "pending",
            "testStrategy": "Execute unit tests for each provider. Verify that mocked network calls are correctly handled and that the `embed` and `embedBatch` methods return expected results for various inputs, including edge cases."
          },
          {
            "id": 2,
            "title": "Develop Unit Tests for Provider Factory and Batching Logic",
            "description": "Create unit tests for the `EmbeddingProviderFactory` to ensure it correctly instantiates providers based on application settings, and for the batching logic within the `pileEmbeddings` module.",
            "dependencies": [],
            "details": "Develop tests for `EmbeddingProviderFactory` to verify it returns the correct `IEmbeddingProvider` instance (Gemini or Ollama) based on the input provider string. Implement unit tests for the `pileEmbeddings` module's `walkAndGenerateEmbeddings` method, focusing on its utilization of the `embedBatch` method and configurable batch size, using mocked provider responses to isolate the batching logic.",
            "status": "pending",
            "testStrategy": "Execute unit tests for the factory, ensuring correct provider instantiation for all valid inputs. Execute unit tests for `pileEmbeddings` batching, verifying batch processing, correct delegation to mocked `embedBatch` calls, and adherence to batch size configurations."
          },
          {
            "id": 3,
            "title": "Develop Unit Tests for embeddings.json Schema Migration",
            "description": "Create unit tests to validate the correctness and integrity of the `embeddings.json` schema migration logic, ensuring data consistency during updates.",
            "dependencies": [],
            "details": "Implement tests that simulate different versions of the `embeddings.json` file (e.g., old schema, new schema, corrupted schema) and verify that the migration logic correctly transforms them to the new schema without data loss or corruption. Cover edge cases, invalid schema scenarios, and ensure backward compatibility where applicable.",
            "status": "pending",
            "testStrategy": "Execute unit tests with various mock `embeddings.json` files representing old and new schemas. Verify the output schema, data integrity, and error handling post-migration."
          },
          {
            "id": 4,
            "title": "Develop Integration Test for Full vectorSearch Path",
            "description": "Create a comprehensive integration test for the entire `vectorSearch` path to ensure it correctly utilizes the selected embedding provider and returns relevant, ranked results.",
            "dependencies": [],
            "details": "Design an end-to-end test scenario that simulates a user query, triggers the embedding generation via the selected provider (Gemini or Ollama), performs the vector search against the indexed embeddings, and validates the relevance and ranking of the returned results. This test should cover the interaction between the `EmbeddingProviderFactory`, `pileEmbeddings` module, and the core search algorithm.",
            "status": "pending",
            "testStrategy": "Execute the integration test in an environment with a configured embedding provider. Verify that the correct provider is used, embeddings are generated, the search process completes successfully, and the search results are accurate and appropriately ranked according to expected relevance."
          },
          {
            "id": 5,
            "title": "Review and Adapt Existing Tests",
            "description": "Review all existing unit and integration tests to identify any that need updates or adaptations due to the introduction of the new embedding provider system and its related changes.",
            "dependencies": [],
            "details": "Go through the existing test suite, particularly tests related to embedding generation, search, data handling, and settings. Modify or extend tests as necessary to ensure they remain valid, cover the new system's interactions, and do not produce false positives or negatives. Ensure test coverage for critical paths is maintained or improved, and remove any redundant tests.",
            "status": "pending",
            "testStrategy": "Run the full existing test suite after adaptations. Verify all tests pass and that the changes correctly reflect the new system architecture without introducing regressions. Document any significant changes made to existing tests."
          }
        ]
      },
      {
        "id": 10,
        "title": "Update Documentation",
        "description": "Update the project's documentation to reflect the new embedding provider options, setup instructions, and usage guidelines.",
        "details": "1. Update the `README.md` file with clear instructions on how to set up and configure the Gemini API key.\n2. Document the process for selecting the desired embedding provider (Gemini or Ollama) in the application settings.\n3. Provide guidance on using the 'Regenerate embeddings' feature.\n4. Add notes on Gemini API quotas, potential costs, and recommended batch sizes for optimal performance.",
        "testStrategy": "Review updated documentation for clarity, accuracy, and completeness. Verify that all new features and configurations are adequately explained.",
        "priority": "low",
        "dependencies": [
          4,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Document Gemini API Key Setup in README",
            "description": "Update the `README.md` file to include clear, step-by-step instructions on how users can obtain, set up, and configure their Gemini API key for the application.",
            "dependencies": [],
            "details": "Focus on providing clear instructions within `README.md` for Gemini API key setup and configuration, including where to obtain the key and how to add it to the application settings.",
            "status": "pending",
            "testStrategy": "Review `README.md` to ensure Gemini API key setup instructions are accurate, easy to follow, and complete."
          },
          {
            "id": 2,
            "title": "Document Embedding Provider Selection Process",
            "description": "Add documentation explaining how users can select their desired embedding provider (Gemini or Ollama) within the application settings, detailing the implications of each choice.",
            "dependencies": [],
            "details": "Document the user interface and steps involved in selecting between Gemini and Ollama as the active embedding provider in the application's settings.",
            "status": "pending",
            "testStrategy": "Verify that the documentation accurately describes the provider selection process and that the instructions are clear and unambiguous."
          },
          {
            "id": 3,
            "title": "Document 'Regenerate Embeddings' Feature Usage",
            "description": "Provide comprehensive guidance on how to use the 'Regenerate embeddings' feature, explaining its purpose, when to use it, and any considerations related to different embedding providers.",
            "dependencies": [],
            "details": "Detail the functionality of the 'Regenerate embeddings' feature, including its location in the UI, what it does, and best practices for its use.",
            "status": "pending",
            "testStrategy": "Check if the 'Regenerate embeddings' feature documentation is clear, covers all relevant aspects, and correctly explains its interaction with provider settings."
          },
          {
            "id": 4,
            "title": "Add Notes on Gemini API Quotas, Costs, and Batching",
            "description": "Include important information regarding Gemini API usage, such as potential quotas, associated costs, and recommendations for optimal batch sizes to manage performance and expenses.",
            "dependencies": [],
            "details": "Add a dedicated section or notes within the documentation covering Gemini API quotas, potential costs, and recommended batch sizes for efficient and cost-effective embedding generation.",
            "status": "pending",
            "testStrategy": "Confirm that the information on Gemini API quotas, costs, and batching is accurate, informative, and provides useful guidance to users."
          },
          {
            "id": 5,
            "title": "Review and Integrate All Documentation Updates",
            "description": "Perform a final review of all updated documentation sections to ensure clarity, accuracy, consistency, and completeness across the entire project documentation. Verify that all new features and configurations are adequately explained and properly integrated.",
            "dependencies": [
              "10.1",
              "10.2",
              "10.3",
              "10.4"
            ],
            "details": "Conduct a thorough review of `README.md` and any other relevant documentation files. Check for consistency in terminology, formatting, and ensure all new information flows logically and is easy to understand. Verify that all aspects of the new embedding provider options are covered.",
            "status": "pending",
            "testStrategy": "Comprehensive review of the entire updated documentation. Verify clarity, accuracy, completeness, and consistency. Ensure all new features and configurations are adequately explained and cross-referenced."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-27T08:56:58.229Z",
      "updated": "2025-08-27T08:57:43.381Z",
      "description": "Tasks for feature-gemini-embeddings context"
    }
  },
  "redesign": {
    "tasks": [
      {
        "id": 1,
        "title": "Epic: Migrate AI integration to Gemini 2.5 Pro",
        "description": "Replace OpenAI with @google/genai, use model `gemini-2.5-pro`, add streaming + JSON mode, secure key handling in Electron main via IPC, update tests/docs.",
        "details": "Scope: main-process client, streaming to renderer, JSON mode for summaries, settings UI for key, repo-wide replacement, security posture per Electron docs.",
        "testStrategy": "Unit tests for client wrapper, IPC handlers, and JSON parsing; integration smoke for streaming UI.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Spike: Set up @google/genai client and streaming in Electron main",
        "description": "Install @google/genai, initialize GoogleGenAI with GEMINI_API_KEY, implement generateContentStream wrapper and log chunks.",
        "details": "Create a main-process module `src/main/ai/gemini.ts` exporting `stream(prompt)` and `json(prompt)` using model `gemini-2.5-pro`. Wire minimal error handling.",
        "testStrategy": "Unit: mock SDK; assert streaming iteration and error handling.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Install @google/genai and scaffold main client",
            "description": "Add dependency, create `src/main/ai/gemini.ts` with model init (`gemini-2.5-pro`).",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Implement generateContentStream wrapper",
            "description": "Expose `stream(prompt)` that yields text chunks; add error handling and abort support.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Implement JSON generation helper",
            "description": "Expose `json(prompt)` using `responseMimeType: 'application/json'` and safe parsing.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 4,
            "title": "Add basic unit tests with SDK mocks",
            "description": "Mock @google/genai and test stream/json helpers for happy/error paths.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 5,
            "title": "Install @google/genai dependency",
            "description": "Add the @google/genai package to the project's dependencies using the project's package manager (npm or yarn).",
            "dependencies": [],
            "details": "Run `npm install @google/genai` or `yarn add @google/genai` and confirm its addition to the `package.json` file. Verify that the project's dependencies can be installed successfully afterward.",
            "status": "done",
            "testStrategy": "Confirm the package is listed in `package.json` and the lock file. Run `npm install` or `yarn install` to ensure no dependency conflicts arise."
          },
          {
            "id": 6,
            "title": "Create gemini.ts module and initialize client",
            "description": "Create the file `src/main/ai/gemini.ts` and implement the initialization logic for the GoogleGenAI client.",
            "dependencies": [],
            "details": "In the new module, import `GoogleGenerativeAI`. Instantiate the client using `new GoogleGenerativeAI(process.env.GEMINI_API_KEY)`. For the spike, the key can be loaded directly from environment variables.\n<info added on 2025-08-26T16:23:21.739Z>\nThe client is initialized with `gemini-2.5-pro` model. Basic structure for `stream` and `json` functions is in place.\n</info added on 2025-08-26T16:23:21.739Z>",
            "status": "done",
            "testStrategy": "Unit test: Mock `process.env.GEMINI_API_KEY` and assert that the `GoogleGenerativeAI` constructor is called with the expected key."
          },
          {
            "id": 7,
            "title": "Implement `stream(prompt)` function",
            "description": "Implement and export an async generator function `stream(prompt)` that calls the Gemini API for streaming responses and logs the chunks.",
            "dependencies": [],
            "details": "Within `gemini.ts`, create the `stream` function. It should get the `gemini-2.5-pro` model from the initialized client, call `generateContentStream({ prompt })`, and iterate through the resulting stream. Log the text from each chunk to the main process console.\n<info added on 2025-08-26T16:24:17.364Z>\nAdditionally, ensure it is an async generator function that yields the text content from each chunk and includes proper error handling with try/catch.\n</info added on 2025-08-26T16:24:17.364Z>",
            "status": "done",
            "testStrategy": "Unit test: Mock the `generateContentStream` method to return a simulated async iterable. Assert that the function iterates correctly and that the content of each chunk is processed as expected."
          },
          {
            "id": 8,
            "title": "Implement `json(prompt)` function",
            "description": "Implement and export an async function `json(prompt)` that calls the Gemini API to get a structured JSON response.",
            "dependencies": [],
            "details": "Within `gemini.ts`, create the `json` function. It should use the `gemini-2.5-pro` model and the `generateContent` method. The implementation should request a JSON response and parse the text output into a JavaScript object before returning it.\n<info added on 2025-08-26T16:25:19.669Z>\n, automatically adds JSON format instruction to prompts if not present, logs the response, and includes proper error handling with try/catch.\n</info added on 2025-08-26T16:25:19.669Z>",
            "status": "done",
            "testStrategy": "Unit test: Mock the `generateContent` method to return a mock response containing a valid JSON string. Assert that the function returns a correctly parsed JavaScript object."
          },
          {
            "id": 9,
            "title": "Add minimal error handling and finalize exports",
            "description": "Wrap the API calls in both the `stream` and `json` functions with try/catch blocks to handle and log potential API or network errors.",
            "dependencies": [],
            "details": "In `gemini.ts`, add `try...catch` blocks around the `generateContentStream` and `generateContent` calls. In the catch block, log a descriptive error message to the console. Ensure both `stream` and `json` functions are properly exported from the module's public interface.\n<info added on 2025-08-26T16:26:09.023Z>\nErrors are re-thrown appropriately from the catch blocks. In addition to `stream` and `json`, `genAI` and `model` functions are also exported from the module's public interface.\n</info added on 2025-08-26T16:26:09.023Z>",
            "status": "done",
            "testStrategy": "Unit test: Mock the SDK methods to throw a specific error. Assert that the error is caught, logged, and that the function gracefully terminates or throws a custom error."
          }
        ]
      },
      {
        "id": 3,
        "title": "IPC bridge: expose Gemini generate + streaming to renderer",
        "description": "Add ipcMain handler and event channel; preload contextBridge exposes invoke and listener; validate sender; no key leakage.",
        "details": "Add channels: `gemini:generate` (invoke) and `gemini:stream` (event). Update preload to expose minimal API. Update window creation to set security flags.",
        "testStrategy": "Unit: mock ipcMain handlers; integration: preload exposure shape.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "ipcMain handler for non-stream JSON",
            "description": "`ipcMain.handle('gemini:generate')` returns JSON or text; validate input and sanitize.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 2,
            "title": "Streaming channel and preload bridge",
            "description": "Send chunks via `webContents.send('gemini:stream', data)`; preload exposes `onGeminiResponse` and `invokeGemini`.",
            "details": "<info added on 2025-08-26T16:29:16.937Z>\nImplemented streaming channel functionality: Added 'gemini:stream' IPC handler that streams chunks via webContents.send() with unique stream IDs. Updated preload.ts to expose onGeminiResponse() listener and invokeGemini()/startStream() methods. Includes proper stream lifecycle management (start/chunk/end/error events) and cleanup functions.\n</info added on 2025-08-26T16:29:16.937Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 3,
            "title": "Type declarations and sender validation",
            "description": "Update `src/renderer/preload.d.ts`; validate sender frame URL is `file:` before serving requests.",
            "details": "<info added on 2025-08-26T16:30:55.209Z>\nSender validation added to Gemini IPC handlers, ensuring `file://` protocol for sender frame URL. TypeScript type definitions (`GeminiResponse`, `GeminiStreamResponse`, `GeminiStreamEvent`) created in `src/main/types/gemini.ts` and used to update handler return types for improved type safety. Preload type declarations are automatically included via `ElectronHandler`.\n</info added on 2025-08-26T16:30:55.209Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 4,
            "title": "Security: narrow preload API surface",
            "description": "Expose only functions needed; no raw ipcRenderer; add input schema validation.",
            "details": "<info added on 2025-08-26T16:32:39.643Z>\nAdded comprehensive input schema validation to all Gemini functions (prompt type/length checks, callback function validation). Added data structure validation for streaming responses before passing to callbacks.\n</info added on 2025-08-26T16:32:39.643Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Renderer: integrate streaming UI and buffering",
        "description": "Adapt chat/editor to consume stream chunks, buffer safely, finalize completion, and handle errors/cancellations.",
        "details": "Add a React hook/useGeminiStream for subscription; ensure UI state transitions match existing behavior; debounce renders.",
        "testStrategy": "Integration: mount component and simulate chunked updates; ensure final text matches.",
        "status": "done",
        "dependencies": [
          "3"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create useGeminiStream hook",
            "description": "Hook subscribes to `onGeminiResponse`, buffers chunks, exposes status and cancel.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Integrate hook into chat/editor",
            "description": "Replace OpenAI streaming usage with Gemini; preserve UX and keyboard shortcuts.",
            "details": "<info added on 2025-08-26T16:37:46.906Z>\nCompleted AIContext.js integration:\n- Updated default pileAIProvider from 'openai' to 'gemini'\n- Updated default model to 'gemini-2.5-pro' \n- Modified setupAi() function to handle Gemini initialization (sets ai type to 'gemini')\n- Updated generateCompletion() to support Gemini streaming with proper event handling\n- Enhanced checkApiKeyValidity() for Gemini validation using invokeGemini test call\n- Preserved existing OpenAI and Ollama compatibility\n- All context values properly exposed to consumers\n</info added on 2025-08-26T16:37:46.906Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Handle errors, cancel, and finalize state",
            "description": "Ensure UI shows errors, allows cancellation, and finalizes on stream end.",
            "details": "<info added on 2025-08-26T16:41:49.285Z>\nCompleted error handling, cancellation, and finalized state UI:\n\nChat Component Enhancements:\n- Added error state management with setError/error state\n- Added canCancel state for showing/hiding cancel button\n- Enhanced onSubmit with try/catch error handling and proper cleanup\n- Added onCancelAI function for cancelling AI requests\n- Added error message UI display with dismissible error banner\n- Enhanced onResetConversation to clear error state\n- Added conditional Cancel AI button in header (replaces Clear chat when streaming)\n\nEditor Component Enhancements:\n- Added canCancelAI state for tracking cancellation availability\n- Enhanced generateAiResponse with better error messages showing actual error details\n- Added cancelAiResponse function with proper cleanup and user feedback\n- Improved error notifications to show specific error messages instead of generic \"AI request failed\"\n- Added proper state management for cancellation throughout AI response lifecycle\n\nBoth components now properly handle Gemini streaming errors, provide cancellation capabilities, and show appropriate UI feedback for all states (streaming, error, completion, cancellation).\n</info added on 2025-08-26T16:41:49.285Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "JSON mode: structured summaries and parsing",
        "description": "Add generationConfig.responseMimeType='application/json' for summary endpoints; robust JSON parse with fallback and user feedback.",
        "details": "Create helper `generateJson(prompt)` with schema-in-prompt; surface parse errors with toasts; add typing to parsed objects.",
        "testStrategy": "Unit: valid/invalid JSON responses; UI: shows error toast when invalid.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Define JSON prompt templates",
            "description": "Create strict, minimal JSON schemas and prompts for summaries and metadata.",
            "details": "<info added on 2025-08-26T16:46:33.505Z>\n- Created `SummaryResponse` and `MetadataResponse` interfaces for parsed JSON output.\n- Added `JSONTemplateResponse` union type for type safety of parsed responses.\n</info added on 2025-08-26T16:46:33.505Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Add parse utility with safe fallback",
            "description": "Gracefully handle invalid JSON; provide error messages and default values.",
            "details": "<info added on 2025-08-26T16:49:36.086Z>\nCompleted safe JSON parsing utility with comprehensive error handling:\n\nMain Parser Module (jsonParser.ts):\n- Created comprehensive JSONParseError enum with error types\n- Implemented ParseResult interface with success/error structure\n- Added validateSummaryResponse() and validateMetadataResponse() functions\n- Created cleanJsonString() to handle common JSON formatting issues (markdown blocks, boundaries)\n- Implemented parseSummaryResponse() and parseMetadataResponse() with length limits\n- Added safeParseJson() generic function with template-specific validation\n- Created DEFAULT_VALUES fallbacks for both template types\n\nRenderer Utilities (jsonHelper.js):\n- Added generateStructuredResponse() with error callback support\n- Created formatErrorMessage() for user-friendly error display\n- Implemented validateResponseCompleteness() for data quality checks\n- Added createErrorToast() for notification integration\n- Provided getDefaultResponse() fallback mechanism\n\nIPC Enhancement:\n- Enhanced GeminiResponse type with parseWarning field\n- Updated generate-json handler to detect fallback usage\n- Added warning detection for invalid responses\n\nExample Implementation:\n- Created JsonExample.jsx component demonstrating full error handling workflow\n- Shows toast notifications for errors and warnings\n- Provides complete reference implementation\n\nThe system now gracefully handles invalid JSON, provides user feedback, and continues functioning with fallback values.\n</info added on 2025-08-26T16:49:36.086Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Settings: Gemini API key input + validation",
        "description": "Add UI to store/validate GEMINI_API_KEY (main process storage) and block AI actions when missing/invalid.",
        "details": "Reword settings from OpenAI to Gemini; add key test ping; never expose key to renderer logs; update README copy.",
        "testStrategy": "Unit: validation logic; UI: disables actions without key; e2e: happy path store + use.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Settings UI updates and copy",
            "description": "Rename OpenAI references to Gemini; adjust help text and validation messages.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Key storage and validation flow",
            "description": "Store key in main process; add a test request; block actions when invalid/missing.",
            "details": "<info added on 2025-08-26T17:54:01.121Z>\nCompleted key storage and validation flow implementation:\n\nKey Storage Enhancement:\n- Modified Gemini client to use stored API key from electron-settings instead of environment variables\n- Added initializeGemini() function to properly initialize client with stored key\n- Updated stream() and json() functions to ensure client is initialized before use\n- Added re-initialization when API key is updated via set-ai-key handler\n\nValidation Flow:\n- Existing checkApiKeyValidity() in AIContext already properly validates Gemini keys by making test API call\n- Key storage uses electron's safeStorage for encrypted storage in main process\n- Settings UI properly calls setKey() when saving changes\n- Delete key functionality properly removes stored credentials\n\nThe system now properly stores, validates, and uses Gemini API keys with secure main process storage and automatic client re-initialization when keys change.\n</info added on 2025-08-26T17:54:01.121Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "Repo-wide replacement: remove OpenAI, wire Gemini services",
        "description": "Remove `openai` package and usages; replace with Gemini service calls in contexts and handlers; update typings.",
        "details": "Search and replace: chat/completions â†’ generateContent/stream; update prompt builders; revisit rate-limit/backoff handlers.",
        "testStrategy": "Run full test suite; smoke run on all AI features; ensure no OpenAI imports remain.",
        "status": "done",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Remove OpenAI deps and imports",
            "description": "Uninstall openai, update package.json/locks; remove imports across codebase.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Replace service/context calls with Gemini",
            "description": "Refactor usage to gemini service; adjust error and backoff logic.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Security & CSP: enforce Electron best practices",
        "description": "Verify contextIsolation, nodeIntegration, preload exposure, and add/adjust CSP. Validate IPC sender origins; sanitize inputs.",
        "details": "Audit BrowserWindow creation, preload API surface, and enable strict CSP for renderer assets.",
        "testStrategy": "Manual audit checklist + unit tests for preload API shape; verify no node APIs in renderer.",
        "status": "done",
        "dependencies": [
          "2",
          "3"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Enforce security flags and narrow preload",
            "description": "Audit BrowserWindow flags; minimize preload API surface; add schema validation.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Add/adjust CSP",
            "description": "Add CSP meta or headers in dev/prod; restrict remote content and eval.",
            "details": "<info added on 2025-08-26T19:12:09.204Z>\nImplemented CSP hardening and consolidation: Removed meta CSP from renderer HTML (header-only now). Added environment-specific CSP in main process (dev vs prod). `script-src` updated to drop 'unsafe-inline' (allowing 'unsafe-eval' only in dev). `style-src` allows 'unsafe-inline' for CSS injection. `connect-src` allows ws/http localhost in dev and restricts to Google API in prod. Added `frame-ancestors 'none'` and `worker-src 'self' blob:`. Validated BrowserWindow flags remain secure (contextIsolation on, nodeIntegration off).\n</info added on 2025-08-26T19:12:09.204Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Tests and CI adjustments",
        "description": "Update unit/integration tests for Gemini workflows; ensure CI scripts keep lint, type-check, packaging, and tests green.",
        "details": "Add mocks for @google/genai; update jest setup if needed; ensure streaming tests are deterministic.",
        "testStrategy": "Jest unit tests for wrappers; integration tests for streaming buffer and JSON parsing.",
        "status": "done",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add @google/genai mocks for Jest",
            "description": "Create stable mocks for SDK streaming and JSON responses.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Unit tests for main helpers",
            "description": "Test stream and JSON helpers (happy/error/abort).",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Integration tests for renderer streaming",
            "description": "Simulate chunked updates and verify buffering/finalization.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Docs & release notes",
        "description": "Update README, AGENTS.md, and in-app copy to reference Gemini; add migration notes and release highlights.",
        "details": "Document settings changes, model defaults, and limitations; include references to Google docs.",
        "testStrategy": "Docs review checklist; ensure commands and paths resolve.",
        "status": "done",
        "dependencies": [
          "7",
          "8",
          "9"
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Supabase Project Setup & Database Schema Implementation",
        "description": "Initialize the Supabase project, configure the database, and implement all core tables as defined in the PRD.",
        "details": "Create the Supabase project. Implement `users`, `user_profiles`, `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments` tables. Ensure `id`, `created_at`, `updated_at` columns are present where applicable. Add initial indexes for common query patterns (e.g., `user_id` on `piles`, `pile_id` on `posts`).",
        "testStrategy": "Verify all tables are created with correct schemas and column types. Insert sample data and confirm data integrity. Check that initial indexes are applied.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Supabase Project",
            "description": "Create a new Supabase project instance and obtain necessary API keys and configuration details.",
            "dependencies": [],
            "details": "Log in to the Supabase dashboard, create a new project, and note down the project URL and `anon` public key. This establishes the foundational environment for the database and other services.",
            "status": "done",
            "testStrategy": "Verify the project is accessible via the Supabase dashboard and API keys are available in the project settings."
          },
          {
            "id": 2,
            "title": "Implement Core Tables with Base Columns",
            "description": "Create the initial set of core database tables as defined, ensuring `id`, `created_at`, and `updated_at` columns are present where applicable.",
            "dependencies": [
              "11.1"
            ],
            "details": "Create `users`, `user_profiles`, `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments` tables. For each, include an `id` column (UUID primary key), `created_at` (timestamp with default now()), and `updated_at` (timestamp with update trigger) columns where appropriate.",
            "status": "done",
            "testStrategy": "Verify all specified tables exist in the Supabase database schema with the correct base columns and data types. Insert a sample row into each table to confirm column functionality."
          },
          {
            "id": 3,
            "title": "Define Table Relationships and Foreign Keys",
            "description": "Establish the relationships between the core tables by adding foreign key constraints to ensure data integrity and referential consistency.",
            "dependencies": [
              "11.2"
            ],
            "details": "Add foreign key constraints, for example: `user_profiles.user_id` to `users.id`, `piles.user_id` to `users.id`, `pile_members.user_id` to `users.id` and `pile_members.pile_id` to `piles.id`, `posts.user_id` to `users.id` and `posts.pile_id` to `piles.id`, `post_replies.user_id` to `users.id` and `post_replies.post_id` to `posts.id`, `post_tags.post_id` to `posts.id`, `post_highlights.post_id` to `posts.id`, and `attachments.post_id` to `posts.id`.",
            "status": "done",
            "testStrategy": "Attempt to insert data that violates foreign key constraints (e.g., a post with a non-existent user_id) to confirm they are correctly enforced. Verify relationships are visible and correct in the Supabase schema visualizer."
          },
          {
            "id": 4,
            "title": "Implement Database Row Level Security (RLS) Policies",
            "description": "Configure Row Level Security policies for all core tables to control data access based on user roles and ownership, ensuring data privacy and security.",
            "dependencies": [
              "11.3"
            ],
            "details": "Enable RLS for `users`, `user_profiles`, `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments`. Define policies that allow users to read/write their own data, and access data within piles they are members of, as appropriate for each table's context.",
            "status": "done",
            "testStrategy": "Test data access from different authenticated users (e.g., user A trying to read user B's private profile or a post in a pile they are not a member of) and unauthenticated users to ensure RLS policies are correctly applied and prevent unauthorized access."
          },
          {
            "id": 5,
            "title": "Add Initial Database Indexes",
            "description": "Create initial indexes on frequently queried columns to optimize database performance for common access patterns.",
            "dependencies": [
              "11.3"
            ],
            "details": "Add indexes for common query patterns as specified: `user_id` on `piles` and `pile_id` on `posts`. Additionally, consider adding indexes on other foreign key columns like `user_id` on `user_profiles`, `post_id` on `post_replies`, etc., to improve join performance.",
            "status": "done",
            "testStrategy": "Verify that the specified indexes are present in the database schema. Run simple queries that utilize these indexed columns and, if possible, analyze query plans to confirm index usage and performance improvement."
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Supabase Authentication (Email/Password & Google OAuth)",
        "description": "Integrate Supabase Auth for user sign-up, sign-in, and session management using email/password and Google OAuth.",
        "details": "Configure Supabase Auth in the project. Enable Email/Password and Google OAuth providers. Implement client-side logic using `supabase-js` for user registration, login, logout, and session persistence. Securely store authentication tokens within the Electron application.",
        "testStrategy": "Test user registration, login, and logout with both email/password and Google accounts. Verify session persistence across app restarts. Confirm secure token storage.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Supabase Project & Client",
            "description": "Set up the Supabase project, enable Email/Password and Google OAuth providers in the Supabase dashboard, and initialize the `supabase-js` client in the Electron application.",
            "dependencies": [],
            "details": "Obtain Supabase URL and Anon Key. Configure these in the Electron app's environment or configuration files. Enable Email/Password and Google OAuth in the Supabase Auth settings within the Supabase dashboard.",
            "status": "done",
            "testStrategy": "Verify Supabase client initializes without errors in the Electron app's console. Confirm Email/Password and Google OAuth providers are visibly enabled in the Supabase dashboard's authentication settings."
          },
          {
            "id": 2,
            "title": "Implement Email/Password Sign-up, Sign-in, and Logout",
            "description": "Develop the client-side logic using `supabase-js` for user registration, login, and logout specifically for email/password authentication.",
            "dependencies": [
              "12.1"
            ],
            "details": "Implement functions for `supabase.auth.signUp({ email, password })`, `supabase.auth.signInWithPassword({ email, password })`, and `supabase.auth.signOut()`. Design and integrate basic UI components (forms, buttons) for these authentication actions within the Electron application.",
            "status": "done",
            "testStrategy": "Test user registration with a new email and password. Verify successful login and logout. Attempt login with incorrect credentials (wrong email, wrong password) and verify error handling. Confirm user data appears in Supabase Auth users table."
          },
          {
            "id": 3,
            "title": "Integrate Google OAuth Sign-in",
            "description": "Implement the client-side logic to allow users to sign in using their Google accounts via Supabase OAuth.",
            "dependencies": [
              "12.1"
            ],
            "details": "Implement `supabase.auth.signInWithOAuth({ provider: 'google' })`. Handle the OAuth redirect flow appropriate for an Electron application, which may involve using a custom protocol handler or an in-app browser window for the authentication process and capturing the callback.",
            "status": "done",
            "testStrategy": "Test sign-in with a valid Google account. Verify successful authentication and the creation of a user session. Confirm the user's Google profile information is correctly retrieved and associated with the Supabase user."
          },
          {
            "id": 4,
            "title": "Implement Session Management and Persistence",
            "description": "Develop the logic to manage user sessions, including refreshing tokens and ensuring session persistence across application restarts.",
            "dependencies": [
              "12.2",
              "12.3"
            ],
            "details": "Utilize `supabase-js`'s built-in session management capabilities. Implement mechanisms to listen for authentication state changes (`supabase.auth.onAuthStateChange`). Ensure the user's session is re-established automatically when the Electron application restarts, relying on `supabase-js`'s internal storage or a custom secure storage solution.",
            "status": "done",
            "testStrategy": "Log in using both Email/Password and Google OAuth. Close and reopen the Electron application and verify the user remains logged in without needing to re-authenticate. Test session refresh after a period of inactivity or token expiration."
          },
          {
            "id": 5,
            "title": "Securely Store Authentication Tokens",
            "description": "Implement a secure method for storing authentication tokens within the Electron application to prevent unauthorized access.",
            "dependencies": [
              "12.4"
            ],
            "details": "Research and implement a secure storage solution for Electron, such as `electron-store` with encryption, `keytar` (for OS-level keychain/credential manager integration), or a custom encrypted file storage mechanism. Ensure that access tokens, refresh tokens, and other sensitive session data are not stored in plain text.",
            "status": "done",
            "testStrategy": "Verify that authentication tokens are stored securely (e.g., not readable in plain text from the file system or developer tools). Test that tokens can be retrieved correctly for session persistence. Attempt to manually tamper with stored tokens and verify the application handles it gracefully (e.g., forces re-login or invalidates the session)."
          }
        ]
      },
      {
        "id": 13,
        "title": "Develop Core Sync Infrastructure & Offline Queue",
        "description": "Establish the foundational client-side sync mechanism and an offline queue for pending operations to support offline-first functionality.",
        "details": "Design and implement a local data cache (e.g., SQLite or IndexedDB) to mirror Supabase data. Create an 'offline queue' to store all CUD (Create, Update, Delete) operations when the application is offline. Implement a mechanism to detect network connectivity changes and automatically replay queued operations to Supabase upon reconnection.",
        "testStrategy": "Simulate offline mode: create/edit/delete entries, then go online and verify successful sync. Test with multiple operations queued. Ensure data consistency after sync.",
        "priority": "high",
        "dependencies": [
          11,
          12
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Local Data Cache",
            "description": "Design and implement the client-side local data cache to mirror Supabase data, providing basic CRUD operations for local storage.",
            "dependencies": [],
            "details": "Choose a local storage solution (e.g., SQLite for Electron/mobile, IndexedDB for web). Define the schema for `posts` and `piles` tables locally, mirroring their Supabase structure. Implement methods for creating, reading, updating, and deleting records within this local cache.",
            "status": "done",
            "testStrategy": "Verify local CRUD operations function correctly. Ensure data persistence across application restarts and that the local cache accurately reflects the intended schema."
          },
          {
            "id": 2,
            "title": "Develop Offline Operations Queue",
            "description": "Create a mechanism to capture and store all CUD (Create, Update, Delete) operations when the application is offline, preparing them for later synchronization.",
            "dependencies": [
              "13.1"
            ],
            "details": "Design a dedicated structure (e.g., a table within the local data cache) to store pending CUD operations. Each queued item should include the operation type (Create, Update, Delete), target table, relevant data/payload, a unique identifier, and a timestamp. Implement an interception layer for application CUD calls to push operations to this queue when offline.",
            "status": "done",
            "testStrategy": "Simulate offline mode. Perform various CUD operations (create, edit, delete) and verify they are correctly stored in the offline queue. Check queue content for accuracy and completeness of operation details."
          },
          {
            "id": 3,
            "title": "Implement Network Connectivity Detection",
            "description": "Develop a robust mechanism to detect changes in network connectivity status (online/offline) to trigger appropriate sync behaviors.",
            "dependencies": [],
            "details": "Implement listeners for browser `online`/`offline` events or use a more sophisticated method (e.g., periodic pings to a known endpoint) to determine actual connectivity. Provide a clear API or event system to notify other parts of the application about connectivity status changes.",
            "status": "done",
            "testStrategy": "Toggle network connectivity (Wi-Fi, airplane mode, disconnect/reconnect Ethernet) and verify the application accurately detects and reports online/offline status changes in real-time."
          },
          {
            "id": 4,
            "title": "Develop Queued Operations Replay Logic",
            "description": "Implement the core logic to automatically replay all stored offline CUD operations to Supabase upon network reconnection.",
            "dependencies": [
              "13.1",
              "13.2",
              "13.3"
            ],
            "details": "Upon detecting an 'online' state, iterate through the offline queue. For each queued operation, execute the corresponding CUD operation against Supabase. Handle successful operations by removing them from the queue. Implement basic error handling for failed operations during replay (e.g., retry mechanism, marking as failed). Ensure operations are replayed in a logical order (e.g., creation before updates/deletions on the same entity).",
            "status": "done",
            "testStrategy": "Simulate offline mode, perform multiple CUD operations (create, update, delete across different entities). Go online and verify all queued operations are successfully replayed to Supabase and removed from the queue. Check Supabase for data consistency and accuracy."
          },
          {
            "id": 5,
            "title": "Implement Initial Conflict Resolution Strategy",
            "description": "Establish a basic conflict resolution strategy for replayed operations to ensure data consistency during sync, specifically 'last-write-wins'.",
            "dependencies": [
              "13.4"
            ],
            "details": "Implement a 'last-write-wins' strategy for conflicts that may arise when replaying queued operations. This involves ensuring that the most recent change (based on a `last_modified_at` timestamp or version) takes precedence. For the initial implementation, this means the replayed local change will overwrite the server's state if a conflict occurs on the same record during replay.",
            "status": "done",
            "testStrategy": "Simulate a conflict scenario: go offline, modify a record. While offline, modify the *same* record directly in Supabase (e.g., via another client or direct DB access). Go online and verify the 'last-write-wins' strategy correctly resolves the conflict, with the expected change (local or remote, based on timestamp) persisting."
          }
        ]
      },
      {
        "id": 14,
        "title": "Implement Row Level Security (RLS) Policies",
        "description": "Define and apply Row Level Security policies to all relevant tables to ensure user data isolation and secure access.",
        "details": "Enable RLS on `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments` tables. Write policies to ensure users can only access their own data or data from piles they are explicitly members of. Test policies rigorously to prevent unauthorized data access.",
        "testStrategy": "Attempt to access data belonging to other users or unshared piles from a logged-in user. Verify that RLS correctly denies access. Test various user roles and permissions.",
        "priority": "high",
        "dependencies": [
          11,
          12
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Enable RLS on all specified tables",
            "description": "Activate Row Level Security for `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments` tables in the database.",
            "dependencies": [],
            "details": "Execute `ALTER TABLE <table_name> ENABLE ROW LEVEL SECURITY;` for each of the seven specified tables: `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments`.",
            "status": "done",
            "testStrategy": "Verify that RLS is enabled for each table by querying `pg_policies` or `information_schema.tables` in the database."
          },
          {
            "id": 2,
            "title": "Implement RLS policies for direct user ownership",
            "description": "Create RLS policies that allow users to access rows where they are the direct owner (e.g., `user_id = auth.uid()`).",
            "dependencies": [
              "14.1"
            ],
            "details": "Create `CREATE POLICY` statements for `SELECT`, `INSERT`, `UPDATE`, `DELETE` operations on tables where data is directly owned by a user (e.g., `posts`, `attachments`, `piles` if a user owns a pile directly) using `auth.uid() = user_id` or similar conditions.",
            "status": "done",
            "testStrategy": "As user A, create data. Log in as user B and attempt to read/update/delete user A's data. Verify access is denied. Log in as user A and verify full access to own data."
          },
          {
            "id": 3,
            "title": "Implement RLS policies for pile membership",
            "description": "Develop RLS policies that grant access to data associated with piles where the current user is an explicit member.",
            "dependencies": [
              "14.1"
            ],
            "details": "Develop `CREATE POLICY` statements for `SELECT`, `INSERT`, `UPDATE`, `DELETE` operations on tables associated with piles (e.g., `posts`, `post_replies`, `post_tags`, `post_highlights`, `attachments`, `pile_members`) that check if `auth.uid()` is a member of the relevant `pile_id` using subqueries or `EXISTS` clauses against the `pile_members` table.",
            "status": "done",
            "testStrategy": "Create a pile and add user A and user B as members. User A creates a post in that pile. Log in as user B and verify access to user A's post within the shared pile. Log in as user C (not a member) and verify access is denied."
          },
          {
            "id": 4,
            "title": "Consolidate and apply all RLS policies to target tables",
            "description": "Combine and apply all defined RLS policies (user ownership and pile membership) to ensure comprehensive and correct access control across all specified tables.",
            "dependencies": [
              "14.2",
              "14.3"
            ],
            "details": "Review and combine the user ownership and pile membership policies. For tables requiring both access types, merge conditions using `OR`. Apply these final, comprehensive policies to all seven specified tables: `piles`, `pile_members`, `posts`, `post_replies`, `post_tags`, `post_highlights`, and `attachments`.",
            "status": "done",
            "testStrategy": "Perform a final review of all `CREATE POLICY` statements for syntax, logical correctness, and complete coverage across all specified tables and operations."
          },
          {
            "id": 5,
            "title": "Conduct comprehensive RLS policy testing and validation",
            "description": "Execute a thorough testing strategy to verify that all RLS policies correctly enforce data isolation and secure access, covering various user roles and access scenarios.",
            "dependencies": [
              "14.4"
            ],
            "details": "Execute the parent task's test strategy: 'Attempt to access data belonging to other users or unshared piles from a logged-in user. Verify that RLS correctly denies access. Test various user roles and permissions.' This involves developing and running a suite of test cases for `SELECT`, `INSERT`, `UPDATE`, `DELETE` operations across all tables under different user contexts (owner, pile member, non-member).",
            "status": "done",
            "testStrategy": "Develop and execute a comprehensive test plan including: 1. Verifying successful access for owners and pile members. 2. Confirming denial of access for unauthorized users (other users' private data, unshared piles). 3. Testing edge cases, such as data without a direct `user_id` or `pile_id` if applicable, and different user roles."
          }
        ]
      },
      {
        "id": 15,
        "title": "Implement Post & Pile Data Synchronization",
        "description": "Develop the core logic for synchronizing `posts` and `piles` data between the local cache and Supabase, including initial fetch and incremental updates.",
        "details": "Implement CRUD operations for `posts` and `piles` that interact with both the local cache and Supabase. Handle the initial data fetch from Supabase to populate the local cache. Implement incremental updates for new, modified, or deleted entries. Initially, use a 'last-write-wins' strategy for conflict resolution.",
        "testStrategy": "Create, update, and delete posts/piles on one device and verify immediate sync to another. Test with large number of entries. Verify data consistency across devices after sync.",
        "priority": "high",
        "dependencies": [
          13,
          14
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Initial Supabase Data Fetch & Local Cache Population",
            "description": "Develop the logic to perform a one-time, comprehensive fetch of all user-accessible `posts` and `piles` from Supabase. This data will then be used to fully populate and initialize the local cache, establishing the baseline for synchronization.",
            "dependencies": [],
            "details": "Query Supabase for all `posts` and `piles` relevant to the current user, respecting RLS policies. Store the fetched data efficiently in the local cache, ensuring the cache accurately reflects the remote state at the time of initialization.",
            "status": "done",
            "testStrategy": "Verify that after initial fetch, the local cache contains all expected data from Supabase for the logged-in user. Test with empty and populated Supabase databases."
          },
          {
            "id": 2,
            "title": "Develop Local Cache CRUD Operations for Posts & Piles",
            "description": "Create the core Create, Read, Update, and Delete (CRUD) functionalities for `posts` and `piles` within the local cache. These operations will serve as the primary interface for the application to interact with cached data, independent of Supabase.",
            "dependencies": [],
            "details": "Implement functions such as `createPostLocal(postData)`, `getPostLocal(id)`, `updatePostLocal(id, newData)`, `deletePostLocal(id)` for both `posts` and `piles`. Ensure these operations maintain the integrity and structure of the local cache.",
            "status": "done",
            "testStrategy": "Perform various CRUD operations directly on the local cache and verify that data is correctly stored, retrieved, modified, and deleted without external dependencies."
          },
          {
            "id": 3,
            "title": "Implement Local-to-Supabase Write-Through & Basic Conflict Handling",
            "description": "Develop the mechanism to automatically push local `posts` and `piles` CRUD operations to Supabase. This subtask includes implementing the initial 'last-write-wins' strategy for local changes being written to Supabase, ensuring local modifications are persisted remotely.",
            "dependencies": [
              "15.2"
            ],
            "details": "When a `create`, `update`, or `delete` operation occurs in the local cache (via 15.2), trigger a corresponding operation on Supabase. For updates, if a conflict is detected (e.g., remote version is older), the local change will overwrite the remote (last-write-wins from local perspective).",
            "status": "done",
            "testStrategy": "Create, update, and delete posts/piles locally and verify that these changes are immediately reflected in Supabase. Simulate a conflict where a local change overwrites a remote change."
          },
          {
            "id": 4,
            "title": "Implement Supabase-to-Local Cache Incremental Sync (Pull Mechanism)",
            "description": "Develop the logic to periodically or reactively pull incremental updates (new, modified, or deleted `posts` and `piles`) from Supabase and stage them for application to the local cache. This ensures the local cache stays aware of changes originating from other clients or direct Supabase interactions.",
            "dependencies": [
              "15.1",
              "15.2",
              "15.3"
            ],
            "details": "Implement a mechanism (e.g., a background process, a listener for Supabase Realtime events, or a polling strategy) to detect and fetch changes in `posts` and `piles` tables on Supabase. Store these incoming changes temporarily before applying them to the local cache.",
            "status": "done",
            "testStrategy": "Make changes directly in Supabase (or from another client) and verify that the local client detects and fetches these changes. Test with new, updated, and deleted entries."
          },
          {
            "id": 5,
            "title": "Implement Last-Write-Wins Conflict Resolution & Local Cache Reconciliation",
            "description": "Implement the core 'last-write-wins' conflict resolution strategy for merging the incremental updates pulled from Supabase (from 15.4) into the local cache. This ensures that the local cache is consistently updated, with remote changes taking precedence in case of conflicts.",
            "dependencies": [
              "15.4"
            ],
            "details": "Process the staged incoming changes from Supabase. For each change, compare it with the current local cache state. If a conflict is detected (e.g., the same item was modified both locally and remotely), the remote version (the 'last write') will overwrite the local version. Reconcile the local cache to reflect these merged changes.",
            "status": "done",
            "testStrategy": "Simulate simultaneous edits on the same post/pile from different devices. Verify that the remote change correctly overwrites the local change, and the local cache reflects the 'last-write-wins' outcome. Test with various conflict scenarios (create/create, update/update, delete/update)."
          }
        ]
      },
      {
        "id": 16,
        "title": "Develop 'Enable Cloud Sync' UI & Migration Wizard",
        "description": "Create the user interface for enabling cloud sync and a guided migration wizard for existing local data.",
        "details": "Design and implement a 'Cloud Sync' section in the application settings. Include an 'Enable Supabase Sync' toggle. Develop a multi-step migration wizard for existing users to upload their local piles to Supabase, including pile selection, estimated size, a progress bar, and a verification step comparing local vs. cloud data.",
        "testStrategy": "Test the entire migration flow with various local data sizes. Verify progress bar accuracy and data integrity post-migration. Ensure rollback capabilities function correctly if issues arise.",
        "priority": "high",
        "dependencies": [
          15
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop 'Cloud Sync' Settings UI & Toggle",
            "description": "Create the dedicated 'Cloud Sync' section within the application settings, including the 'Enable Supabase Sync' toggle.",
            "dependencies": [],
            "details": "Implement the UI for the 'Cloud Sync' section in the application settings. Include an 'Enable Supabase Sync' toggle switch. Design the layout and styling for this settings area.",
            "status": "done",
            "testStrategy": "Verify the 'Cloud Sync' section appears correctly in settings. Test the toggle switch's visual state changes and ensure it can be interacted with."
          },
          {
            "id": 2,
            "title": "Implement Migration Wizard Shell & Navigation",
            "description": "Create the multi-step wizard component, define its overall structure, and manage navigation between steps.",
            "dependencies": [
              "16.1"
            ],
            "details": "Develop the main wizard container component. Implement forward/backward navigation buttons and logic. Define the sequence of steps (e.g., Welcome, Pile Selection, Upload, Verification, Complete). Implement the initial launch logic when the 'Enable Supabase Sync' toggle is activated and local data is detected.",
            "status": "done",
            "testStrategy": "Verify the wizard launches correctly from the settings toggle. Test navigation between empty steps (forward/back). Ensure the wizard can be closed or cancelled at any point."
          },
          {
            "id": 3,
            "title": "Develop Pile Selection & Size Estimation Step",
            "description": "Create the wizard step allowing users to select local data piles for migration and display their estimated sizes.",
            "dependencies": [
              "16.2"
            ],
            "details": "Implement the UI for displaying a list of local data 'piles' (e.g., checkboxes for selection). Develop logic to read local data and calculate the estimated size for each selected pile. Display the total estimated size to the user.",
            "status": "done",
            "testStrategy": "Test with various numbers of local piles (none, few, many). Verify accurate size estimation for selected piles. Ensure selection/deselection updates the total estimated size correctly."
          },
          {
            "id": 4,
            "title": "Implement Data Upload & Progress Monitoring Step",
            "description": "Develop the wizard step responsible for uploading selected local data to Supabase and displaying a real-time progress bar.",
            "dependencies": [
              "16.3"
            ],
            "details": "Implement the core logic for iterating through selected local piles and uploading their contents to Supabase. Integrate with the underlying sync infrastructure to perform the upload. Design and implement a dynamic progress bar that updates in real-time. Include error handling for upload failures and options for retry.",
            "status": "done",
            "testStrategy": "Simulate various data sizes for upload (small, medium, large). Verify progress bar accuracy and smooth updates. Test network interruptions during upload and ensure graceful handling/resumption or clear error messages."
          },
          {
            "id": 5,
            "title": "Develop Data Verification & Completion Step",
            "description": "Create the final wizard step to verify data integrity post-upload and provide a completion or rollback option.",
            "dependencies": [
              "16.4"
            ],
            "details": "Implement logic to compare the uploaded data in Supabase with the original local data to ensure consistency and completeness. Display a verification status (success/failure). Provide options to finalize the migration, retry, or initiate a rollback if issues are detected. Design the success/failure message UI.",
            "status": "done",
            "testStrategy": "Test with successful migrations, verifying data integrity by comparing local and cloud data. Simulate verification failures and ensure appropriate error messages and rollback options are presented. Test rollback functionality to ensure local data is restored correctly."
          }
        ]
      },
      {
        "id": 17,
        "title": "Implement Conflict Resolution Strategy",
        "description": "Refine the conflict resolution mechanism for simultaneous edits or offline changes, providing user-friendly options.",
        "details": "Enhance the 'last-write-wins' strategy with more sophisticated conflict detection. Design and implement a UI/UX for users to be notified of conflicts and potentially choose between conflicting versions or merge changes, especially for collaborative editing scenarios.",
        "testStrategy": "Simulate simultaneous edits on the same post from different devices. Test offline edits followed by online edits. Verify the conflict resolution UI is clear and functional.",
        "priority": "medium",
        "dependencies": [
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Conflict Detection Rules & Data Model",
            "description": "Research and define specific rules for detecting conflicts beyond 'last-write-wins' (e.g., field-level changes, semantic conflicts). Determine necessary data model enhancements (e.g., versioning, change tracking metadata) to support these rules.",
            "dependencies": [],
            "details": "Investigate strategies like operational transformation (OT) or conflict-free replicated data types (CRDTs) for inspiration, but focus on a practical approach for the current system. Specify how `posts` and `piles` data will be compared to identify conflicts and what metadata is needed to track versions.",
            "status": "pending",
            "testStrategy": "Review and validate the defined rules and data model with stakeholders and technical leads. Ensure they cover anticipated conflict scenarios and are feasible to implement within the Supabase ecosystem."
          },
          {
            "id": 2,
            "title": "Implement Server-Side Conflict Detection Logic",
            "description": "Develop and integrate the backend logic to detect conflicts based on the defined rules during data synchronization. This involves comparing incoming changes with the current server state and identifying discrepancies.",
            "dependencies": [
              "17.1"
            ],
            "details": "Modify Supabase Functions or database triggers/procedures to perform detailed conflict detection when `posts` or `piles` are updated. This logic should identify *what* fields are conflicting and *which* versions are involved, preparing the data for client-side resolution.",
            "status": "pending",
            "testStrategy": "Unit test the conflict detection logic with various simulated conflicting scenarios (e.g., simultaneous edits on the same field, different fields, offline edits). Verify that conflicts are correctly identified and categorized by the backend."
          },
          {
            "id": 3,
            "title": "Design Conflict Resolution User Interface (UI/UX)",
            "description": "Create wireframes, mockups, and user flows for how conflicts will be presented to users and how they can resolve them. This includes notification, version comparison, and selection options.",
            "dependencies": [
              "17.2"
            ],
            "details": "Design the UI/UX for a conflict notification banner or modal. Define the layout for presenting conflicting versions (e.g., side-by-side, 'Your changes' vs. 'Other's changes'). Specify options like 'Keep My Version', 'Discard My Version', 'Take Other's Version', and a potential 'Merge' option.",
            "status": "pending",
            "testStrategy": "Conduct internal reviews of wireframes/mockups. Gather feedback from potential users or UI/UX experts on clarity, usability, and effectiveness of the proposed conflict notification and resolution flow."
          },
          {
            "id": 4,
            "title": "Implement Client-Side Conflict Notification & Version Selection UI",
            "description": "Develop the front-end components to notify users of detected conflicts and allow them to choose between their local version and the server's conflicting version.",
            "dependencies": [
              "17.2",
              "17.3"
            ],
            "details": "Implement the UI elements designed in 17.3. This includes displaying conflict alerts, fetching and presenting the conflicting versions (e.g., current local vs. server's latest), and handling user input to select a preferred version. Integrate this with the client-side sync logic.",
            "status": "pending",
            "testStrategy": "Simulate conflicts (e.g., by manually creating conflicting data or using test scripts) and verify that the UI correctly notifies the user, presents the conflicting versions clearly, and allows successful selection of one version, which then syncs correctly."
          },
          {
            "id": 5,
            "title": "Implement Advanced Conflict Merging Functionality",
            "description": "Develop the more sophisticated functionality for users to merge conflicting changes, potentially involving a diff viewer and granular selection of changes.",
            "dependencies": [
              "17.2",
              "17.3",
              "17.4"
            ],
            "details": "Implement a UI component that can display a diff between conflicting versions of a post or pile. Provide controls for users to selectively accept or reject individual changes or lines of content. This will require client-side logic to apply the merged changes back to the local data and then synchronize.",
            "status": "pending",
            "testStrategy": "Test the merge UI with complex conflicting content, including additions, deletions, and modifications in different parts of a post. Verify that the diff viewer accurately highlights changes and that users can successfully merge and save the desired outcome, which is then correctly synchronized."
          }
        ]
      },
      {
        "id": 18,
        "title": "Implement Real-time Post Editing & Presence",
        "description": "Integrate Supabase Realtime for live synchronization of post content, live cursor positions, and user presence within shared piles.",
        "details": "Utilize `supabase.channel` to subscribe to `posts` changes within a specific pile for real-time content updates. Implement a mechanism for live cursor positions (e.g., using a dedicated presence channel or a temporary table). Develop UI to display presence indicators (who is currently online and viewing/editing a pile).",
        "testStrategy": "Test real-time editing from multiple clients simultaneously. Verify cursor positions update accurately. Confirm presence indicators reflect active users in a pile.",
        "priority": "high",
        "dependencies": [
          15
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Develop Pile Sharing & Member Management UI",
        "description": "Create the user interface and backend logic for sharing piles with other users and managing collaborators' permissions.",
        "details": "Implement functionality to generate shareable links for piles, allowing users to invite others. Develop a UI for managing pile members, including inviting, removing, and assigning roles/permissions (e.g., read-only, editor) using the `pile_members` table. Ensure RLS policies are respected.",
        "testStrategy": "Test sharing a pile with different permission levels. Verify invited users can access/edit based on their role. Confirm the owner can remove members and revoke access.",
        "priority": "high",
        "dependencies": [
          14,
          18
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Backend API for Shareable Links & Initial Invitations",
            "description": "Develop the server-side logic and API endpoints required to generate unique shareable links for piles and process initial invitations when users accept these links.",
            "dependencies": [],
            "details": "Implement API endpoints for creating secure, unique shareable links for a given pile, including options for default permission levels. Develop backend logic to store link metadata and to validate and process invitations, adding users to the `pile_members` table with the specified role upon link acceptance.",
            "status": "pending",
            "testStrategy": "Test link generation for various piles and permission levels. Verify that accepting a link correctly adds a user to the `pile_members` table with the intended role."
          },
          {
            "id": 2,
            "title": "Develop UI for Shareable Link Generation & Display",
            "description": "Create the user interface elements that allow pile owners to generate shareable links and easily copy them, including options for initial access permissions.",
            "dependencies": [],
            "details": "Design and implement a UI component (e.g., a button, a modal within the pile view) that enables a pile owner to generate a shareable link. Display the generated link prominently for copying and provide UI controls for setting the initial permission level (e.g., read-only, editor) for users joining via this link.",
            "status": "pending",
            "testStrategy": "Verify the UI correctly generates and displays links. Test setting different initial permission levels. Ensure the link is easily copyable."
          },
          {
            "id": 3,
            "title": "Implement Backend API for Member CRUD & Role Management",
            "description": "Develop the server-side logic and API endpoints for directly managing pile members, including adding, removing, and updating their roles and permissions.",
            "dependencies": [],
            "details": "Implement API endpoints for direct management of pile members: adding specific users to a pile (e.g., by ID or email), removing users from a pile, and updating the roles/permissions (e.g., 'read-only', 'editor') of existing members within the `pile_members` table.",
            "status": "pending",
            "testStrategy": "Test adding, removing, and updating roles for various users on different piles via the API. Verify `pile_members` table reflects changes accurately."
          },
          {
            "id": 4,
            "title": "Develop UI for Pile Member Management Dashboard",
            "description": "Create a dedicated user interface for pile owners to view, invite, remove, and modify the roles of collaborators on their piles.",
            "dependencies": [],
            "details": "Design and implement a dedicated UI screen or modal (e.g., a 'Manage Collaborators' panel) where the pile owner can view a list of all current members, see their assigned roles, and perform actions such as inviting new users (e.g., by username/email lookup), removing existing members, and changing their roles (e.g., from read-only to editor).",
            "status": "pending",
            "testStrategy": "Verify the UI accurately displays members and roles. Test inviting new members, removing existing members, and changing roles through the UI. Confirm UI updates reflect backend changes."
          },
          {
            "id": 5,
            "title": "Implement RLS Policies & Conduct End-to-End Testing",
            "description": "Ensure all pile sharing and member management operations strictly adhere to Row Level Security policies and conduct comprehensive testing across all functionalities.",
            "dependencies": [],
            "details": "Implement and rigorously verify Row Level Security (RLS) policies on the `pile_members` table and any related tables to ensure that users can only perform actions (view, edit, manage) consistent with their assigned roles and pile ownership. Conduct comprehensive end-to-end integration testing covering shareable link generation, invitation acceptance, direct member invitation, member removal, role changes, and access validation for all defined roles (owner, editor, read-only) to confirm RLS is correctly enforced across the entire system.",
            "status": "pending",
            "testStrategy": "Simulate various user roles (owner, editor, read-only, uninvited) and attempt all operations (view pile, edit pile, generate link, invite, remove, change role). Verify that only authorized actions succeed and unauthorized actions are blocked by RLS."
          }
        ]
      },
      {
        "id": 20,
        "title": "Integrate Supabase Storage for Attachments",
        "description": "Implement secure file upload, download, and management for attachments using Supabase Storage buckets.",
        "details": "Create `pile-attachments` and `user-avatars` storage buckets in Supabase. Implement client-side logic for securely uploading images and other files, associating them with `posts` via the `attachments` table. Develop functionality to download and display these attachments. Configure RLS for storage buckets to ensure only authorized users can access files.",
        "testStrategy": "Upload various file types and sizes. Verify successful upload, secure storage, and correct display/download. Test RLS by attempting to access unauthorized files.",
        "priority": "high",
        "dependencies": [
          11,
          14,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Implement Full-Text Search on Post Content",
        "description": "Add efficient full-text search capabilities across all user's journal entries using PostgreSQL's built-in features.",
        "details": "Configure a `tsvector` column on the `posts` table, automatically updating it on content changes using a trigger. Implement a search API endpoint that utilizes `to_tsquery` for fast and relevant full-text search across all posts accessible to the user. Optimize search queries for performance.",
        "testStrategy": "Perform searches with various keywords, phrases, and special characters. Verify search results are accurate and returned within the performance requirements (<1 second). Test search across large datasets.",
        "priority": "medium",
        "dependencies": [
          11,
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure tsvector Column and GIN Index",
            "description": "Add a `tsvector` column to the `posts` table and create a GIN index on it to enable efficient full-text search.",
            "dependencies": [],
            "details": "Modify the `posts` table schema to include a new `tsvector` column, e.g., `search_vector`. This column will store the processed text for full-text search. Create a GIN index on this `search_vector` column for optimal query performance.",
            "status": "pending",
            "testStrategy": "Verify the `posts` table schema includes the new `tsvector` column and that the GIN index is successfully created. Check index properties and ensure it's a GIN index."
          },
          {
            "id": 2,
            "title": "Implement tsvector Update Trigger",
            "description": "Create a PostgreSQL trigger to automatically update the `tsvector` column whenever a post's content is inserted or updated.",
            "dependencies": [],
            "details": "Develop a PostgreSQL function and an associated trigger that fires `BEFORE INSERT OR UPDATE` on the `posts` table. This trigger will populate the `search_vector` column using `to_tsvector` on the `content` column (and potentially other relevant text fields like `title`) with an appropriate text search configuration (e.g., 'english').",
            "status": "pending",
            "testStrategy": "Create a new post and update an existing post. Verify that the `search_vector` column is correctly populated and updated with the processed text after each operation. Test with various content types and lengths."
          },
          {
            "id": 3,
            "title": "Backfill Existing Post tsvector Data",
            "description": "Populate the `tsvector` column for all existing journal entries in the `posts` table to make historical data searchable.",
            "dependencies": [],
            "details": "Write and execute a one-time SQL script to iterate through all existing `posts` and populate their `search_vector` column using `to_tsvector(config, content)`. This ensures all historical data is immediately searchable upon feature deployment.",
            "status": "pending",
            "testStrategy": "After backfill, query a representative sample of old posts directly to ensure their `search_vector` column contains the expected data. Perform a few test searches on old content to confirm they return results."
          },
          {
            "id": 4,
            "title": "Develop Full-Text Search API Endpoint",
            "description": "Create a new API endpoint that accepts search queries and returns relevant posts using PostgreSQL's full-text search capabilities, respecting user access.",
            "dependencies": [],
            "details": "Implement a server-side API endpoint (e.g., a Supabase Function or API route) that takes a search string. This endpoint will use `to_tsquery` to convert the search string and then query the `posts` table, joining on the `search_vector` column using the `@@` operator. Ensure the query respects Row Level Security (RLS) to only return posts accessible to the authenticated user.",
            "status": "pending",
            "testStrategy": "Perform searches with various keywords, phrases, and special characters through the API. Verify that results are accurate, relevant, and only include posts the authenticated user has access to. Test with different user accounts to confirm RLS is enforced."
          },
          {
            "id": 5,
            "title": "Optimize Search Performance and Relevance",
            "description": "Refine search queries and database configuration to ensure full-text searches are fast and return highly relevant results, meeting performance requirements.",
            "dependencies": [],
            "details": "Analyze query plans for the search endpoint. Implement optimizations such as using `ts_rank` for result ordering, considering different `ts_config` settings (e.g., `english`, `simple`), and potentially adding partial indexing or other PostgreSQL FTS features. Ensure search queries consistently return results within the specified performance target (<1 second) even with large datasets.",
            "status": "pending",
            "testStrategy": "Conduct performance tests with large datasets and complex queries, measuring query execution times. Verify that search results are ordered by relevance and that the system can handle concurrent search requests efficiently. Test against the <1 second performance requirement."
          }
        ]
      },
      {
        "id": 22,
        "title": "Develop Sync Status Indicators & Error Handling UI",
        "description": "Create clear visual indicators for sync status, connection status, and user-friendly error messages for sync-related issues.",
        "details": "Implement UI elements (e.g., sidebar icons, status bar indicators) to visually communicate 'synced', 'syncing', 'offline', and 'conflict' states. Design and implement clear, actionable error messages and recovery options for network issues, sync failures, or data conflicts.",
        "testStrategy": "Simulate various network conditions (offline, slow, intermittent). Verify UI indicators accurately reflect status. Trigger sync errors and confirm user-friendly error messages are displayed with guidance.",
        "priority": "medium",
        "dependencies": [
          13,
          15
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Sync Status UI Elements & States",
            "description": "Define the visual appearance, iconography, and placement for 'synced', 'syncing', 'offline', and 'conflict' states across various UI components.",
            "dependencies": [],
            "details": "Create detailed mockups or wireframes specifying the visual representation (icons, colors, animations) for 'synced' (e.g., green checkmark), 'syncing' (e.g., spinning arrow), 'offline' (e.g., cloud with slash), and 'conflict' (e.g., warning triangle). Determine optimal placement for these indicators, such as a global status bar, sidebar icons, or specific pile/item indicators.",
            "status": "pending",
            "testStrategy": "Conduct internal design review sessions with UX/UI specialists and product stakeholders to ensure clarity, consistency, and user-friendliness of the proposed visual states."
          },
          {
            "id": 2,
            "title": "Implement Visual Sync Status Indicators",
            "description": "Develop and integrate the UI components to display 'synced', 'syncing', 'offline', and 'conflict' states based on the approved design.",
            "dependencies": [
              "22.1"
            ],
            "details": "Implement the actual UI elements (e.g., React components, CSS styles) for sidebar icons and status bar indicators. Integrate these components with the client-side sync infrastructure (Task 13) to subscribe to and react to real-time status changes (e.g., `syncService.onStatusChange('synced')`). Ensure smooth visual transitions between states.",
            "status": "pending",
            "testStrategy": "Develop unit tests for UI components to verify correct rendering for each state. Simulate various network conditions (e.g., disconnect/reconnect, slow network) and verify that the UI indicators accurately reflect 'synced', 'syncing', 'offline' states. Manually trigger a simulated conflict event from Task 13 to verify the 'conflict' indicator."
          },
          {
            "id": 3,
            "title": "Design User-Friendly Error Messages & Recovery Flows",
            "description": "Define the content, tone, and structure for user-facing error messages related to sync failures, network issues, and data conflicts, including actionable recovery options.",
            "dependencies": [],
            "details": "Draft specific, concise, and empathetic error messages for common sync-related issues such as 'Network connection lost. Please check your internet connection and try again.', 'Sync failed: Server unavailable. We're working on it, please try again later.', 'Data conflict detected. Please review your changes or discard local edits.' For each error, define clear, actionable recovery steps and design the UI patterns (e.g., toast notifications, modal dialogs with buttons, inline alerts) for their presentation.",
            "status": "pending",
            "testStrategy": "Conduct user feedback sessions or A/B tests on drafted error messages and recovery options to ensure they are easily understandable, helpful, and reduce user frustration. Review with content writers for tone and clarity."
          },
          {
            "id": 4,
            "title": "Implement Error Handling UI & Integration Logic",
            "description": "Develop the UI components to display the designed error messages and integrate the logic to trigger them based on sync-related events from the core infrastructure.",
            "dependencies": [
              "22.3"
            ],
            "details": "Implement the UI components for displaying error messages (e.g., a global notification system, specific modal components). Integrate with the core sync infrastructure (Task 13) to listen for specific error events (e.g., `syncService.onError('network_offline')`, `syncService.onError('server_error')`, `syncService.onConflict(conflictDetails)`). Implement the logic to map these events to the appropriate designed error messages and display them, including any recovery action buttons.",
            "status": "pending",
            "testStrategy": "Develop integration tests that simulate various error conditions (e.g., network timeout during a sync operation, API rate limit error, a forced data conflict) and verify that the correct, user-friendly error messages are displayed with the appropriate recovery options. Ensure error messages are dismissible or provide clear next steps."
          },
          {
            "id": 5,
            "title": "End-to-End Integration, Refinement & Comprehensive Testing",
            "description": "Perform comprehensive end-to-end testing, integrate all sync status and error handling components, and refine the overall user experience.",
            "dependencies": [
              "22.2",
              "22.4"
            ],
            "details": "Conduct full system integration testing to ensure seamless interaction between sync status indicators and error messages. Verify that indicators update correctly after an error is resolved or a recovery action is taken. Perform performance testing to ensure UI updates are responsive. Gather final user feedback and address any remaining bugs or UX issues to ensure a robust and intuitive sync experience.",
            "status": "pending",
            "testStrategy": "Execute a comprehensive test plan covering all sync scenarios, including offline operations, intermittent connectivity, concurrent edits leading to conflicts, and various error recovery paths. Verify data consistency, UI responsiveness, and user guidance throughout the entire sync lifecycle."
          }
        ]
      },
      {
        "id": 23,
        "title": "Implement Data Export Functionality",
        "description": "Allow users to export their complete journal data from Supabase in a common, accessible format.",
        "details": "Develop a feature that enables users to export their entire journal data (posts, attachments metadata, pile structure) in a user-friendly format such as JSON or a collection of Markdown files. Consider using Supabase Functions to generate the export file server-side and store it temporarily in an `exports` storage bucket for download.",
        "testStrategy": "Export data for users with varying amounts of content and attachments. Verify the exported data is complete, correctly formatted, and easily importable/readable. Test the download process.",
        "priority": "low",
        "dependencies": [
          15,
          20
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Performance Optimization & Load Testing",
        "description": "Conduct comprehensive performance testing, identify bottlenecks, and optimize database queries, real-time subscriptions, and sync logic.",
        "details": "Perform load testing for concurrent users and large datasets to identify performance bottlenecks. Optimize SQL queries by adding necessary composite indexes and refining existing ones. Fine-tune real-time subscriptions and client-side sync logic to meet performance requirements (e.g., <2s sync latency, <500ms query response, <100ms real-time latency).",
        "testStrategy": "Use profiling tools to identify slow queries. Conduct stress tests with simulated users. Monitor Supabase metrics for database performance. Verify all performance requirements are met under load.",
        "priority": "high",
        "dependencies": [
          15,
          18,
          20,
          21
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Comprehensive End-to-End Testing & Documentation",
        "description": "Conduct final end-to-end testing, user acceptance testing (UAT), and create user documentation for all new cloud features.",
        "details": "Develop and execute comprehensive end-to-end test plans covering all user stories, technical requirements, and edge cases. Organize beta testing with a group of users to gather feedback. Create clear and concise user documentation, including guides for cloud sync, collaboration, data migration, and troubleshooting common issues.",
        "testStrategy": "Execute full regression test suite. Conduct UAT sessions and collect feedback. Review all documentation for accuracy, clarity, and completeness. Verify all success criteria from the PRD are met.",
        "priority": "high",
        "dependencies": [
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Comprehensive End-to-End Test Plans",
            "description": "Design detailed test plans for all new cloud features, covering user stories, technical requirements, edge cases, and a full regression suite.",
            "dependencies": [],
            "details": "Based on the Product Requirements Document (PRD) and implemented features (cloud sync, collaboration, data migration, error handling, etc.), create a comprehensive suite of end-to-end test cases. Include scenarios for successful operations, error conditions, edge cases, and a full regression test suite to ensure existing functionality is not broken.",
            "status": "pending",
            "testStrategy": "Conduct peer review of test plans for coverage, accuracy, and completeness. Ensure traceability of test cases to user stories and technical requirements."
          },
          {
            "id": 2,
            "title": "Execute End-to-End & Regression Testing",
            "description": "Perform the developed end-to-end and regression tests, log defects, and verify all success criteria are met.",
            "dependencies": [
              "25.1"
            ],
            "details": "Execute the test plans developed in subtask 25.1. Document all test results, including passed, failed, and blocked tests. Log any defects found with detailed steps to reproduce, expected results, and actual results. Track defect resolution and retest fixes. Verify that all success criteria from the PRD are met.",
            "status": "pending",
            "testStrategy": "Monitor test execution progress and coverage. Track defect rates and resolution times. Generate comprehensive test reports summarizing coverage and defect status."
          },
          {
            "id": 3,
            "title": "Conduct User Acceptance Testing (UAT) & Gather Feedback",
            "description": "Organize and facilitate beta testing with a group of users to gather feedback on new cloud features and validate user experience.",
            "dependencies": [
              "25.2"
            ],
            "details": "Recruit a diverse group of beta users. Prepare a UAT environment and provide clear instructions and scenarios. Conduct UAT sessions, collect qualitative and quantitative feedback through surveys, interviews, and direct observation. Analyze feedback to identify usability issues, bugs, and areas for improvement.",
            "status": "pending",
            "testStrategy": "Ensure UAT participants represent target user demographics. Document all feedback received. Prioritize feedback and create actionable items for development teams."
          },
          {
            "id": 4,
            "title": "Draft User Documentation for Cloud Features",
            "description": "Create initial drafts of clear and concise user documentation, including guides for cloud sync, collaboration, data migration, and troubleshooting common issues.",
            "dependencies": [],
            "details": "Write comprehensive guides covering how to enable and use cloud sync, share piles and manage collaborators, perform data migration, and troubleshoot common issues related to cloud features. Focus on clarity, conciseness, and user-friendliness, targeting the end-user perspective.",
            "status": "pending",
            "testStrategy": "Conduct internal review of drafted documentation for grammatical correctness, clarity, and adherence to established style guides and terminology."
          },
          {
            "id": 5,
            "title": "Review, Refine, and Finalize User Documentation",
            "description": "Conduct a thorough review of all drafted user documentation for accuracy, clarity, completeness, and incorporate feedback before final publication.",
            "dependencies": [
              "25.3",
              "25.4"
            ],
            "details": "Review the drafted documentation (from 25.4) against the implemented features and UAT feedback (from 25.3). Ensure all steps are accurate, screenshots are up-to-date, and language is consistent. Refine content based on clarity and completeness checks. Obtain final approval from product and technical leads. Publish the finalized documentation to the appropriate channels.",
            "status": "pending",
            "testStrategy": "Cross-reference documentation content with actual application behavior. Conduct a final proofread. Verify all success criteria related to documentation (accuracy, clarity, completeness) are met."
          }
        ]
      },
      {
        "id": 26,
        "title": "Fix Google OAuth Popup UX and Session Transfer",
        "description": "Address critical UX issues in Google OAuth, ensuring the popup window closes automatically and the authentication state is correctly transferred to the main window for a seamless user experience.",
        "details": "The current Google OAuth flow successfully authenticates users but fails to provide a smooth UX due to the popup window not closing automatically and the main window not reflecting the authenticated state. The fix will involve:\n1.  **Popup Window Management**:\n    *   Modify the OAuth initiation to open the Google sign-in flow in a new `BrowserWindow` managed by the Electron main process.\n    *   In the main process, attach a `webContents.on('did-navigate')` or `webContents.on('will-redirect')` listener to the popup `BrowserWindow`.\n    *   Upon detection of the Supabase OAuth callback URL (containing `access_token`, `refresh_token`, etc.), capture the full URL.\n    *   Parse the URL to extract the session tokens.\n    *   Immediately close the popup `BrowserWindow` using `popupWindow.close()`.\n2.  **Authentication State Transfer**:\n    *   After extracting the session tokens in the main process, use `supabase.auth.setSession({ access_token, refresh_token })` if the Supabase client is managed in the main process.\n    *   Alternatively, send the extracted `access_token` and `refresh_token` to the main window's renderer process via IPC (e.g., `ipcMain.send(mainWindow.webContents, 'auth:session-update', { accessToken, refreshToken })`).\n    *   In the main window's renderer process, listen for the `auth:session-update` IPC message.\n    *   Upon receiving the message, update the Supabase client's session using `supabase.auth.setSession()` or trigger a manual session refresh (`supabase.auth.refreshSession()`) to ensure the `onAuthStateChange` listener is triggered.\n    *   Ensure the UI components in the main window (e.g., sign-in button, user profile, piles list) react to the updated authentication state.\n    *   Consider using `localStorage` directly if `supabase.auth.setSession` is not directly available or if the `onAuthStateChange` listener is expected to pick up changes from `localStorage` automatically.",
        "testStrategy": "1. Initiate the Google OAuth sign-in process from the main application window.\n2. Verify that a new popup window appears for Google authentication.\n3. Complete the Google authentication successfully.\n4. Observe that the popup window automatically closes immediately after successful authentication, without displaying the app's home page.\n5. Verify that the main application window instantly transitions from showing the sign-in button to displaying the authenticated user's profile and 'piles' list.\n6. Log out from the application and repeat the entire process to ensure consistent behavior.\n7. Test scenarios where the user closes the popup manually before completing authentication (main window should remain unauthenticated).\n8. Verify error handling if session transfer fails (e.g., tokens are invalid or missing).",
        "status": "in-progress",
        "dependencies": [
          3,
          8
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify deep link registration (macOS) and add success/error logging",
            "description": "Confirm `app.setAsDefaultProtocolClient('pilebintang')` succeeds at runtime on macOS. Add explicit logs for registration result and add `app.on('open-url')` diagnostics to confirm the callback is received. Ensure logs are concise and gated behind a debug flag.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 2,
            "title": "Implement deep-link handling for Windows/Linux via `second-instance` argv",
            "description": "Use `app.requestSingleInstanceLock()` and the `second-instance` event to parse argv for `pilebintang://auth-callback?...`, route to the same resolver (close popup, resolve callback URL), and add diagnostics. Document the expected argv format for different shells.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 3,
            "title": "Add comprehensive OAuth popup logging across all navigation events",
            "description": "Instrument the OAuth popup with debug logs for `will-navigate`, `will-redirect`, `did-redirect-navigation`, `did-navigate`, `did-fail-load`, `did-fail-provisional-load`, and `setWindowOpenHandler`. Each log should include the event name, URL, and whether the flow resolved or was ignored.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 4,
            "title": "Harden dev HTTP callback close conditions for app origin and /auth/callback",
            "description": "Expand app-origin detection to include http/https with dev host/port and path, and intercept navigation to `/auth/callback` immediately. Ensure the popup closes before the SPA renders and starts API calls inside the popup.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 5,
            "title": "Update Supabase Allowed Redirect URLs and verify PKCE compatibility",
            "description": "In the Supabase dashboard, add both `pilebintang://auth-callback` and your dev URL (e.g., `http://localhost:1212/auth/callback`) to Allowed Redirect URLs. Confirm the supabase-js version supports `auth.exchangeCodeForSession(authCode)`; adjust to `getSessionFromUrl` if needed.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 6,
            "title": "Enhance renderer diagnostics and manual callback URL exchange helper",
            "description": "Add logs for `OAuth result` including whether `callbackUrl` is present (mask sensitive params). Show user-friendly error if `success === false`. Optionally add a debug input to paste a callback URL and trigger `exchangeCodeForSession` manually to recover.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          },
          {
            "id": 7,
            "title": "Prepare production packaging and docs for deep link OAuth",
            "description": "Document deep link registration and expected behavior in packaged builds. Verify the installer/packager registers `pilebintang://` and test the OAuth flow end-to-end in a packaged app. Include troubleshooting tips for users.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-26T16:05:18.194Z",
      "updated": "2025-08-27T17:59:30.569Z",
      "description": "Tasks for research context",
      "renamed": {
        "from": "research",
        "date": "2025-08-27T16:34:54.213Z"
      }
    }
  }
}